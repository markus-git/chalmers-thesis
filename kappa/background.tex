%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .--~*teu.
%  dF     988Nx
% d888b   `8888>
% ?8888>  98888F
%  "**"  x88888~
%       d8888*`
%     z8**"`   :
%   :?.....  ..F
%  <""888888888~
%  8:  "888888*
%  ""    "**"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[../paper.tex]{subfiles}
\begin{document}

\chapter{Background}
\label{background}

High demands for efficiency under resource constraints have greatly influenced the development of embedded systems, both in terms of programming practice and architecture. Today, we see embedded systems consisting of everything between general purpose processors and application specific integrated circuits (ASICs).

% Processors are highly programmable systems but often inefficient in terms of power consumption and performance. In contrast, an ASIC implements a fixed function and can therefore provide good power and performance characteristics, but any changes to its functionality requires a new circuit to be designed. 

While processors and ASICs represent two extremes of available architectures, field programmable gate arrays (FPGAs) have found a good middle-ground and provide the best of both worlds: they are close to hardware and can be reprogrammed~\cite{bacon2013}. Modern FPGAs also contain various discrete components and co-processors, which together with their good performance per Watt ratio, have seen them increasingly used in high-performance, computationally intensive systems~\cite{mcmillan2014}.

% FPGAs typically consists of a large array of configurable logic blocks, connected by programmable interconnects.

While modern FPGAs show great promise as a prototypical system for heterogeneous computing, their adoption has been slowed by the fact that they're difficult to program. The logic blocks are usually programmed in a hardware description language, while the embedded processors are programmed in a low level dialect of C or even assembler. Furthermore, the various components of a modern FPGA may support different intrinsics, leading to incompatibilities between the code they can execute even if they are programmed in the same language.

% The use of low level languages is primarily driven by the desire to access the full potential of a processor or its memory system. However, such languages also forces its developers to focus on low level implementation details rather than the high level specification of the algorithm they are implementing.

One of the benefits of low level code is that it gives programmers fine control over the aforementioned system capabilities. However, this control comes at a cost, as the programmers must exercise this right during the entire design process. So the problem of implementing an algorithm has become a problem of implementing an algorithm for a specific component.

The issues with low level languages are magnified for heterogeneous systems, as the developer must specify both its hardware and software parts and how they communicate; ideally she would like to experiment with various choices of what to put in hardware and what in software. Low-level languages provide little support for such design exploration, and rewriting code intended for one component to another is typically a major undertaking.

Many of the issues faced in heterogeneous computing with low-level languages stem from a lack of abstractions. Some of these languages' modularity problems come as a direct result of the fine grained control they provide, as it will inevitably tie programs to the architecture of its system. Some issues of, for instance, functionality and architecture come as a indirect consequence of the lack of abstractions. Ideally, such issues would be treated separately, as they allow for the creation of small, reusable libraries that provide solutions to these issues, and then combined in a modular fashion to solve larger problems.

% For instance, we would like to treat functionality like parallelism as a separate design decision. In imperative languages like C, this is however quite difficult to do. On the other hand, HDLs assume programs are parallel by default, and extra care has to be taken when ordering operations.

% Issues with differences in language design between C and HDLs can also be ascribed to a lack of abstractions. Ideally, issues of functionality, architecture or parallelism would be treated separately. As being able to do so would allow us to create reusable libraries that provide solutions to these issues, and then combine them in a modular fashion to solve problems.

In the paper ``Why functional programming matters''~\cite{hughes1989}, Hughes argues that many of the above modularity problems can be address by making use of functional programming. Particularily the glue code that functional programming languages offer, through higher-order functions and lazy evaluation, enables us to build useful combinators.

\section{Functional Programming}
\label{functional}

Functional programming, as the name implies, is based around the application of a function to its arguments. In this programming style, a program is written as a function that accept input and deliver its result. This function itself is defined in terms of smaller functions, which in turn are defined using smaller functions still and, in the end, a function consists of nothing but language primitives, such as built-in functions, variables or constants.

An important distinction between functions in a functional programming language and, say an imperative language like C, is that functions always return the same value when given same arguments. More generally, we say that functional programs have no side effects; functions can safely be evaluated in parallel as long at their data dependencies are satisfied.

A function that accepts other functions as arguments is often referred to as a higher-order function, or a combinator, and provides a useful piece of glue code that lets programmers build complex functions from smaller ones. In Haskell, a functional programming language, a number of such higher-order functions that implement common operations are provided by its standard libraries. One such function is \codei{map} and can be defined as follows:

\begin{code}
map :: (a -> b) -> [a] -> [b]
map f []     = []
map f (x:xs) = f x : map f xs
\end{code}

The first line specifies the type of \codei{map}, because in Haskell, every function is assigned a static type in an effort to attain safer programs. If you by mistake write a program that tries to multiply some integer by a boolean type, it won't even compile and instead warns you about the type error. As for the types themselves, they are a kind of label that every expression has and states what category of operations that the expression belongs in.

A function's type comes after the \codei{::} sign, and in the case of \codei{map}, tells us that its first argument is a function \codei{f :: a -> b} which, given an argument of type \codei{a}, produces a result of type \codei{b}. In addition to the function, \codei{map} also takes a list \codei{xs :: [a]} of element with type \codei{a}, and returns another list of elements with type \codei{b}. As functions cannot have any side effects, we can already make a guess at what this function does and claim that it applies \codei{f} to every element of \codei{xs}.

The second and third line of \codei{map} validates our earlier guess and lists the full definition of the function. Firstly, it says that given an empty list, shown as \codei{[]}, the result is another empty list---there's simply nothing to apply \codei{f} to. Secondly, in the case where \codei{map} is given a non-empty list \codei{x:xs} where \codei{x} is the list head and \codei{xs} its tail, it applies \codei{f} to \codei{x} and concatenates its result with the list made from a recursive call to itself on \codei{xs}.

The usefulness of higher-order functions extend to any emedded language as well, as they can be used to encode common patterns. The co-design language makes extensive use of higher-order functions to build combinator libraries. For example, the \codei{zipWith} function used in our earlier \codei{dotVec} example from section~\ref{intro} is a higher-order function for joining two vectors with a given function.

The other piece of glue code that functional programming languages provides is often referred to as function composition, and enables programs to be glued together. Say that \codei{f} and \codei{g} are two programs, then \codei{g} composed with \codei{f} is written \codei{g <> f} and is a program that, when applied to its input \codei{x}, computes \codei{g (f x)}. In Haskell, we can define function composition as:

\begin{code}
(.) :: (b -> c) -> (a -> b) -> a -> c
(.) g f x = g (f x)
\end{code}

\noindent where parenthesis around the dot implies that function composition is an infix function. While the size of the intermediate result of \codei{f} could spoil the usefulness of composition, functional programming solves this by only evaluating \codei{f} as much as is needed by \codei{g}. This property is referred to as lazy evaluation.

Lazy evaluation in Haskell is not limited to its own functions and values, but extends to functions written in embedded languages as well. For functions in our co-design language, the laziness means that only the parts that contribute to the end result will be part of the function, that is, no unnecessary code will generated or evaluated for a program. As we will see later on, this property is especially useful for vectors as it can guarantee fusion.

So far, we have looked at a few Haskell functions as an introduction to functional programming and talked a bit about how its beneficial properties can help embedded languages. We have yet to make the distinction between regular and embedded Haskell functions. The following section introduces the concept of domain specific languages, and explains what explains what it entails to be an embedded domain specific language in Haskell.

\section{Domain Specific Languages}
\label{domain}

A domain specific language (DSL) is a special-purpose language, tailored to a certain problem and captures the concepts and operations in its domain. For instance, a hardware designer might write in VHDL, while a web-designer that wants to create an interactive web-page would use JavaScript. Both use a language that is specialized to the particular task they have at hand, and both build programs in a form that is familiar to regular programmers; VHDL and JavaScript are both examples of a DSL.

DSLs comes in two fundamentally different forms: external and internal. An external DSL is a first-class language, with its own compiler or interpreter, and often comes with its own ecosystem. The previous VHDL and JavaScript examples both fall into the domain of external DSLs. Internal DSLs are embedded in a host language, and are often referred to as embedded domain specific languages (EDSLs). The co-design, vector and signal languages presented in this thesis are all examples of internal DSLs.

% Embedded languages can have the look and feel of a stand-alone language, but reuse many parts of the host-language's ecosystem and semantics to lower the cost to develop and maintain them.

Haskell, with its static type system, flexible overloading and lazy semantics, has come to host a range of EDSLs~\cite{elliott2003}. For instance, popular libraries for parsing, pretty printing, hardware design and testing have all been embedded in Haskell~\cite{leijen2002, hughes1995, bjesse1998}. These embedded languages come in two different flavors, where the most common one is called shallow embedding and is represented by functions that implements the semantics of the embedded language.

We illustrate how shallow embeddings work in Haskell through a small example over simple, integral expressions:

\begin{code}
type Exp = Int

const :: Int -> Exp
const a = a

times :: Exp -> Exp -> Exp
times a b = a * b
\end{code}

\noindent Expressions, here called \codei{Exp}, are defined as a type synonym for integers, making use of Haskell's \codei{type} keyword. The language around our expressions is defined by two functions, one for integer literals, called \codei{const} and another for multiplication, called \codei{times}. Note we could easily have added more functions, as long as they're part of the domain.

The advantage of a shallow type like \codei{Exp} is that we can quickly calculate its value:

\begin{code}
eval :: Exp -> Int
eval a = a
\end{code}

\noindent Shallowly embedded languages do however perform quite poorly if we wish to compile the language; functions only return values and provide no way to look at the representation of a program. To compile an embedded language it is better to use an intermediate representation for functions, which sits between Haskell and the compiled code. This technique is known as a deep embedding and its functions return an abstract syntax tree that represents the computed value instead of its result.

We reimplement our earlier, shallowly embedded language for integer expressions to make use of a deep embedding instead:

\begin{code}
data Exp = Const Int | Times Exp Exp
\end{code}

\noindent Expressions contain two constructors, one for integer literals and another for multiplication of expressions---Haskell's \codei{data} keyword introduces a new type and its different constructors are separated by a pipe. These constructors forms what is often referred to as a syntax tree, and represents the computations behind an expression.

As the type of expressions have changed from being a synonym to a concrete type, we must also change the functions that comes with it:

% Because the previous incarnation of expressions supported literals and multiplication function, we have given our new data type two constructors that represent these two actions. The language around our expressions changes as we 

\begin{code}
const :: Int -> Exp
const a = Const a

times :: Exp -> Exp -> Exp
times a b = Times a b
\end{code}

\noindent The functions now return a representation of the result rather than the result itself. As a consequence, we cannot add new functions without first extending the \codei{Exp} type. From a users prespective, embedded functions in Haskell are however not that different from their shallow counterparts. In fact, embedded languages can have the look and feel of a stand-alone language, but reuse many parts of the host-language's ecosystem and semantics.

The syntax tree used for a deeply embedded type like \codei{Exp}, while inflexible compared to its shallow version, is what enables functions to inspect, modify, and interpret an expression in order to support, for example, their evaluation:

\begin{code}
eval :: Exp -> Int
eval (Const a)   = a
eval (Times a b) = (eval a) * (eval b)
\end{code}

\noindent Each line of \codei{eval} handles one of the two constructors in \codei{Exp}, translating them into their corresponding Haskell value. Evaluating an expression to its equivalent Haskell value is however not the only supported interpretation of expressions. In fact, we could just as well have compiled the same expression to, say, its corresponding C code.

% The one exception in this translation is variables, since they cannot be translated without knowing the context in which the expression is evaluated---we simply don't know what value it references. As a result, trying to evaluate an expression that contains open variables will yield an error.

% While the representation of embedded language's syntax tree can become quite large and unwieldy as the set supported operations grows, it is necessary to have one if we would like to do any kind of transform over the representation, or if we wish to introduce another interpretation.

While the implementation of shallow and deep embedding are usually at odds, there has been work done in order to combine their benefits~\cite{svenningsson2012}. In our co-design language, we make use of such a combination of deep and shallow embeddings: the core syntax is implemented as a deep embedding, with user facing libraries as shallow embeddings on top. This mixture of embeddings means that our core language is easy to interpret, while the user-facing libraries are able to provide a nice syntax for their functions.

% The benefits of shallow and deeply embedded are both attractive to have for an EDSL, at least in the sense that a shallow embedding can easily add new language features and a deep embedding supports multiple interpretations.

Now that we have grasp of functional programming, what domain specific languages in Haskell are and the ideas behind them, the next section will go through a larger example in order to showcase embedded programming in the co-design language.

\section{Embedded Programming in Haskell}
\label{embedded}

Programming in a functional language like Haskell is different from the imperative style of programming where users express their program as a series of sequential steps. In Haskell, users write their program as a mathematical function, that is, a function from its inputs to its output. This is in contrast to the imperative style of programming, 

% Furthermore, functional programming is typically done compositionally, and its languages provide a rich of operators to support the composition of new functions from smaller ones.

As an example of the above differences, we'll consider a finite impulse response (FIR) filter, one of the two primary types of digital filters used in digital signal processing applications~\cite{oppenheim1989}. The mathematical definition of a FIR filter of rank $N$ is as follows:

% A FIR filter is a filter whose impulse response has a finite duration, since it eventually settles to zero. This is in contrast to infinite impulse response filters, the second filter of the two primary types of digital filters, which may have an infinite impulse response. 

\begin{equation}
y_{n} = b_{0} x_{n} + b_{1} x_{n-1} + \cdots + b_{N} x_{n-N} = \sum_{i=0}^{N} b_{i} x_{n-i}
\end{equation}
\vspace{1mm}

\noindent where $x$ and $y$ are the input and output signals, respectively, and $b_i$ is the value of the impulse response at time instant $i$. The inputs $x_{n-i}$ are sometimes referred to as ``taps'' as they tap into the input signal at various time instants. 

In an imperative language like C, we can implement the FIR filter as:

\begin{code}
void fir(int N, int L, double *b, double *x, double *y)
{
 int j, k;
 double tap[256];
 for(j=0; j<N; j++) tap[j] = 0.0;
 for(j=0; j<L; j++)
 {
  for(k=N; k>1; k--) tap[k-1] = tap[k-2];
  tap[0] = x[j];
  y[j] = 0.0;
  for(k=0; k<N; k++) y[j] += b[k] * tap[k];
 }
}
\end{code}

\noindent Here, $N$ is the filter rank, as before, and $L$ is the size of the input---we assume $N$ will be smaller than $256$. The three variables $b$, $x$, and $y$ point towards arrays containing the coefficient, input, and output, respectively. As for the functions body, the first for-loop initializes the taps while the second for-loop goes through all of the inputs and shifts them onto the taps and computes their impulse response.

Looking at the C code, at first glance it seems to be a good representation of the FIR filter. There are however a few problems with the implementation. For instance, the second of the two inner for-loops that calculates the intermediate result $v$ does so by performing a dot-product of the arrays $b$ and $tap$. Implementing a dot-product in this manner will tie it to our FIR filter, as opposed to a stand-alone function which can be used by many. While it is possible to extract the computation like so:

\begin{code}
double dot(int N, double *xs, double *ys)
{
  double sum = 0;
  for (int i=0; i<N; i++) sum += xs[i] * ys[i];
  return sum;
}
\end{code}

\noindent The function is still specialized to values of type $double$, it assumes $b$ and $tap$ both have at least $N$ elements, and it is not readily compositional since the function cannot be merged with the producers of \codei{xs} or \codei{ys} without looking at its implementation.

A dot product can be implemented in our embedded co-design language, as shown in section~\ref{intro}, using a similar imperative, but not idiomatic, style:

\begin{code}
dotSeq :: Arr Float -> Arr Float -> Program (Exp Float)
dotSeq x y = do
  sum <- initRef 0
  for 0 (min (length x) (length y) $ \ix -> do
    a <- getArr x ix
    b <- getArr y ix
    modifyRef sum $ \s -> s + a * b
  getRef sum
\end{code}

\noindent It does so by making use of the fact that a \codei{Program} is a monad. That is, programs acts as a kind of composable computation descriptions, and together with Haskell's \codei{do} notation, allows us sequence instructions very much like we would do in C.

This version of a dot product is however not without fault. As with the C version, the function is locked to values of \codei{Float}. We could however tackle this problem by making use of Haskell's \codei{Num} class, changing the function's type while leaving its body intact:

\begin{code}
dotSeq :: Num a => Arr a -> Arr a -> Program (Exp a)
\end{code}

\noindent Where the function is now polymorphic in the kind of values it accepts, but also limited to numerical values that supports addition and multiplication. Even with its new type, the function is quite still fragile: the for-loop's length and the array indexing are both handled manually. That is, a single typo in any of these two would break the function but not its type, and any error would first emerge at run-time.

For purely array based computations like the dot product, the idiomatic approach is however to use our vector language and build larger functions from its library of smaller, verified functions. A vector based dot product can be defined succinctly as follows:

\begin{code}
dotVec :: Num a => Vec a -> Vec a -> Exp a
dotVec xs ys = sum (zipWith (*) xs ys)
\end{code}

\noindent Here, the dot product is calculated by first joining the two vectors \codei{xs} and \codei{ys} by element-wise multiplication with \codei{zipWith}, and then reducing the resulting list with \codei{sum}.

The above version of the dot product is not only closer to its mathematical specification than the sequential one, but also sturdier in the sense that its harder for users to make an error. Furthermore, Haskell's lazy evaluation ensures that \codei{dotVec} can be merged freely with the producers of \codei{xs} and \codei{ys}.

For a full implementation of the FIR filter, the vector language is however a bit outside its comfort zone: the library excel at describing array transformations, whereas the filter is described by a recurrence equation where output depends on previous input values. Nevertheless, the vector library does provide a few such recurrence functions, and we use one of them to implement the full filter:

\begin{code}
firVec :: Num a => Vec a -> Vec a -> Program (Arr a)
firVec cs v = recurrenceI (replicate (length bs) 0) v $ \i -> dotVec cs i
\end{code}

\noindent Where \codei{recurrenceI} takes an initial buffer, an input vector to iterate over, and a step function that produces one output at a time from the previous inputs and buffer.

Locking the type signature of \codei{firVec} to, for example, 32-bit signed integers, we can compile the function to C:

\begin{code}
ghci> icompile (firVec (-1...1) (1...10) :: Program (Arr Int32))
Generated C code goes here.
\end{code}

Recurrence functions like \codei{recurrenceI} can handle regular feedback of the FIR filter quite well, but struggles for when the output needs irregular access to earlier inputs. FIR filters are however typically used in digital signal processing applications, and while the filter can be described using vectors, the idiomatic approach for such functions is to instead use our signal processing language. 

The signal processing language is built upon the co-design language and is based on the concept of signals: possibly infinite sequences of values. Like the vector language, idiomatic signal functions are constructed compositionally using smaller functions, but signals also provide a function for introducing unit delays. As an example, we create three signal functions that represents the main components of a FIR filter:

\begin{code}
sums :: Num a => [Sig a] -> Sig a
sums as = foldr1 (+) as

muls :: Num a => [Exp a] -> [Sig a] -> [Sig a]
muls as bs = zipWith (*) (map constant as) bs

dels :: Exp a -> Sig a -> [Sig a]
dels e as = iterate (delay e) as
\end{code}

\noindent That is, a summation and a multiplication with coefficients, forming a dot product together, and a number of successive delays to form the taps. Note that addition and multiplication are lifted to operate element-wise over signals, and \codei{constant} and \codei{delay} are signal functions that introduces a constant signal and a unit delay, respectively. The list operations \codei{foldr1}, \codei{zipWith}, \codei{map}, and \codei{iterate} are all part of Haskell's standard functions.

A full FIR filter can now be expressed as:

\begin{code}
firSig :: Num a => [Exp a] -> Sig a -> Sig a
firSig coeffs = sums . muls coeffs . dels 0
\end{code}

\noindent Which is quite close to the filter's mathematical specification, but also its graphical representation: the input signal is delayed to form the filter's taps, where each ``tap'' is multiplied with a coefficient and then summed together.

Locking the type signature of \codei{firSig} to signals over 32-bit integers, like we did with the vector version, we can compile the function to C:

\begin{code}
ghci> icompileFun (firSig [-1,0,1] :: Sig Int32 -> Sig Int32)
Generated C code goes here.
\end{code}

\section{Summary}

Section~\ref{intro} gave a general introduction to embedded programming and its challenges. Specifically the problem of extracting performance from an embedded system while keeping its power cost down was discussed. Heterogeneous systems was then introduced as a possible response to these problems, where section~\ref{background} mentioned modern FPGAs in particular as a prototypical heterogeneous system of interest.

Heterogeneous systems are not without their own challenges, as the presence of multiple processors raises all of the issues involved with parallel, homogeneous systems. Also, the level of heterogeneity in a system can introduce additional challenges with different system capabilities and development between processors.

% In the design of a heterogeneous systems there is concerns related to any dissimilarities between the embedded components that needs to be address. Components may support different instructions, leading to incompatibilities between the code they can execute even if they're both programmed in the same language. Even the implementation of language features, such as functions, may differ between components. In addition to instructions, components may interpret memory in different ways and have access to various cache structures. These differences imply that a heterogeneous system may have components that are similar in architecture, but have low-level differences that lead to variations in performance and power consumption.

Functional languages was then introduced in section~\ref{functional} as a response to the various modularity issues with using lower-level languages like C or VHDL for heterogeneous systems. Particularly the ``glue code'' of functional languages, that is their higher-order functions and lazy evaluation, was shown to be useful for developing reusable components. Section~\ref{domain} showed how these benefits extended to languages embedded in a functional language like Haskell.

Section~\ref{embedded} went on to introduce our current attempt at bringing the benefits of functional programming languages to the domain of embedded heterogeneous systems with our co-design, vector and signal languages.

%\subsubsection{Interpretations}

%Evaluating an expression to its equivalent Haskell value is not the only supported interpretation of expressions. In fact, we could just as well have compiled the same expression to, say, its corresponding C code. The ability to interpret \codei{Exp} in multiple ways comes from the fact that its a deeply embedded type; operations over \codei{Exp} build an interim data structure that describes the expression. For instance, an expression like \codei{a * b} could result in an abstract syntax tree of \codei{Mult (Var ``a'') (Var ``b'')}, representing the multiplication of the two variables \codei{a} and \codei{b}. A Syntax tree like this one is what enables the compiler to inspect, modify, and interpret an expression in order to support, for example, its compilation into source code or evaluation.

%For the case of evaluating an expression, if we assume the \codei{Exp} is defined as:

%\begin{code}
%data Exp a = Var String | Lit a | Mult (Exp a) (Exp a)
%\end{code}

%\noindent that is, expressions contain three constructors for variables, literals, and multiplication---Haskell's \codei{data} keyword introduces a new type and its constructors are separated by a pipe---then its evaluation can be defined like so:

%\begin{code}
%evaluateExp :: Type a => Exp a -> a
%evaluateExp (Var v)    = error "open expression."
%evaluateExp (Lit i)    = i
%evaluateExp (Mult a b) = (evaluateExp a) * (evaluateExp b)
%\end{code}

%\noindent Each line of \codei{evaluateExp} handles one of the three constructors and translates them into their corresponding Haskell value. The one exception in this translation is variables, since they cannot be translated without knowing the context in which the expression is evaluated---we simply don't know what value it references. As a result, trying to evaluate an expression that contains open variables will yield an error.

%\subsubsection{Vectors}

%The similarities between the implementation of the embedded and regular dot-products are mostly superficial. This is because, the helper functions used in the embedded variant are not the standard Haskell functions for lists, but rather functions provided by our vector library that share the same names:

%\begin{code}
%data Vec a = Vec (Exp Length) (Exp Index -> Exp a)

%zipWith :: (Type a, Type b)
%  => (Exp a -> Exp b -> Exp c)
%  ->  Vec a -> Vec b -> Vec c
%zipWith f a b = map (uncurry f) (zip a b)

%sum :: (Num a, Type a) => Vec a -> Exp a
%sum a = fold (+) 0 a
%\end{code}

%\noindent where both \codei{zipWith} and \codei{sum} are defined with the help of other vector functions.

%What's interesting about vectors is that they're defined in terms of a regular Haskell data type with two components: a length, given by the expression \codei{Exp Length}; and an indexing function \codei{(Exp Index -> a)} that, given an index of type \codei{Exp Index}, looks up the value \codei{a} of the vector. \codei{Index} and \codei{Length} are both integer types in disguise that have been renamed to make them more descriptive.

%Its no coincident that the definition of vectors is similar to their semantics, as \codei{Vec} is an example of a shallowly embedded data type and the very idea of its representation is to model the operations that can be performed on vectors. This characteristic of shallow embeddings makes it easy to add new functions, but it also makes them difficult to interpret in any way other than the intended one. It is because of this deficiency that vectors are based on the deeply embedded \codei{Exp} type; evaluating \codei{Vec} turns it into one big expression which can then be interpreted.

%Compiling the embedded dot-product to C yields the following code:

%\begin{code}
%-- todo: compile dot-product to C and past code here.
%\end{code}

\end{document}
