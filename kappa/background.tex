%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .--~*teu.
%  dF     988Nx
% d888b   `8888>
% ?8888>  98888F
%  "**"  x88888~
%       d8888*`
%     z8**"`   :
%   :?.....  ..F
%  <""888888888~
%  8:  "888888*
%  ""    "**"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[../paper.tex]{subfiles}
\begin{document}

\chapter{Background}
\label{background}

% High demands for efficiency under resource constraints have greatly influenced the development of embedded systems, both in terms of programming practice and architecture.

Today, we see embedded systems consisting of everything between general purpose processors (GPPs) and application specific integrated circuits (ASICs). GPPs and ASICs represent two extremes of available architectures, where field programmable gate arrays (FPGAs) have found a good middle-ground and provide the best of both worlds: they are close to hardware and can be reprogrammed~\cite{bacon2013}. Modern FPGAs also contain various discrete components and co-processors which, together with their good performance per Watt ratio, have seen them increasingly used in high-performance, computationally intensive systems~\cite{mcmillan2014}. While a modern FPGA shows great promise as a prototypical system for heterogeneous computing, its adoption has been slowed by the fact that it is difficult to program.

% Furthermore, the various components of a modern FPGA may support different intrinsics and therefore have incompatibilities between the code they can execute even if they are programmed in the same language.

The logic blocks of an FPGA are usually programmed in a hardware description language, while its co-processors are programmed in some low level dialect of C or even assembler. Low level languages are typically used for embedded systems as they give designers fine control over the system's capabilities. Such fine control does however come at a cost, as the programmers must exercise this right during the entire design process. So the problem of implementing an algorithm has become a problem of implementing an algorithm for a specific system's architecture.

The issues with low level languages are magnified for heterogeneous systems, as the developer must specify both its hardware and software parts and how they communicate; ideally she would like to experiment with various choices of what to put in hardware and what in software. Low-level languages provide little support for such design exploration, and rewriting code intended for one of the FPGA's processing elements to another is typically a major undertaking.

Many of the issues faced in heterogeneous computing with low-level languages stem from a lack of abstractions. Some of these languages' modularity problems come as a direct result of the fine grained control they provide. Other issues of, for instance, functionality and architecture come as an indirect consequence of the lack of abstractions. Ideally, such issues would be treated separately from an algorithm's design.

% as they allow for the creation of small, reusable libraries that provide solutions to these issues, and then combined in a modular fashion to solve larger problems.

In the paper ``Why functional programming matters''~\cite{hughes1989}, Hughes argues that many of the above modularity problems can be addressed by making use of functional programming. Particularly the glue code that functional programming languages offer, through higher-order functions and lazy evaluation, enables us to build useful combinators.

The benefits of a functional programming language are however not limited to software development, as Sheeran shows in her paper ``Hardware Design and Functional Programming: a Perfect Match''~\cite{sheeran2005}. Sheeran exemplifies how a functional language can make it easy to explore and analyze hardware designs in a way that traditional hardware description languages would have found difficult, if not impossible.

Before we go into combining these benefits for heterogeneous computing, an introduction to functional programming and embedded languages is in order.

\section{Functional Programming}
\label{functional}

Functional programming is based around the application of a function to its arguments. In this programming style, a program is written as a function that accepts input and delivers its result. That function itself is defined in terms of smaller functions, which in turn are defined using smaller functions still and, in the end, a function consists of nothing but language primitives.

An important distinction between functions in a functional programming language and, say, an imperative language like C, is that functions always return the same value when given the same arguments. More generally, we say that functional programs have no side effects: functions can safely be evaluated in parallel as long as their data dependencies are satisfied.

% This property that is also quite useful when reasoning about a functions behavior.

A function that accepts other functions as arguments is often referred to as a higher-order function, or a combinator, and provides a useful piece of glue code that lets programmers build complex functions from smaller ones. In Haskell, a number of such higher-order functions are provided by its standard libraries. One such function is \codei{map} and can be defined as follows:

\begin{code}
map :: (a -> b) -> [a] -> [b]
map f []     = []
map f (x:xs) = f x : map f xs
\end{code}

The first line specifies the type of \codei{map}, because in Haskell, every function is assigned a static type in an effort to attain safer programs---if a function tries to multiply an integer with a boolean, the compiler will reject the function and instead point out the type mismatch. In the case of \codei{map}, its type is a function from another function \codei{f :: a -> b} and a list \codei{xs :: [a]} to an element of type \codei{b}. The second and third line of \codei{map} specifies that it, when given an empty list as denoted by \codei{[]}, returns another empty list, and for non-empty lists, applies \codei{f} to the list's head and recursively calls itself on the list's tail.

% As for the types themselves, they are a kind of label that every expression has and states what category of operations that the expression belongs in. 

% As functions cannot have side effects, it is possible to guess what this function does by just looking at its type.

% and claim that it applies \codei{f} to every element of \codei{xs}.

% A function's type comes after the \codei{::} sign, and in the case of \codei{map}, tells us that its first argument is a function \codei{f :: a -> b} which, given an argument of type \codei{a}, produces a result of type \codei{b}. In addition to the function, \codei{map} also takes a list \codei{xs :: [a]} of elements with type \codei{a}, and returns another list of elements with type \codei{b}. As functions cannot have any side effects, we can already make a guess at what this function does and claim that it applies \codei{f} to every element of \codei{xs}.

% The second and third line of \codei{map} validate our earlier guess and list the full definition of the function. Firstly, it says that given an empty list, shown as \codei{[]}, the result is another empty list---there's simply nothing to apply \codei{f} to. Secondly, in the case where \codei{map} is given a non-empty list \codei{x:xs} where \codei{x} is the list head and \codei{xs} its tail, it applies \codei{f} to \codei{x} and concatenates its result with the list made from a recursive call to itself on \codei{xs}.

The usefulness of higher-order functions like \codei{map} comes from their ability to encode common patterns: \codei{map} works for all functions and lists that fit its type signature. Functions like \codei{map} are often referred to as combinators, a style of organizing libraries around a few primitive values and functions for combining them. These combinators allow for complex structures to be built from a small set of pre-verified functions. For example, the earlier dot-product from section~\ref{intro} is composed of two combinators: \codei{zipWith}, a generic way of joining two vectors, and \codei{sum}:

% Both are these functions are in turn defined by smaller functions:

\begin{code}
zipWith :: (a -> b -> c) -> Vec a -> Vec b -> Vec c
zipWith f a b = fmap (uncurry f) $ zip a b

sum :: Vec Int -> Int
sum = fold (+) 0
\end{code}

\noindent \codei{fmap} here is similar to the above \codei{map} but works for vectors instead of lists, \codei{zip} joins two vectors into a single vector of pairs, and \codei{fold} reduces a vector into a scalar value using addition and starting at zero.

% These three functions are all part of the vector library.

The other piece of glue code that functional programming languages provides is often referred to as function composition, and enables programs to be glued together. Say that \codei{f} and \codei{g} are two programs, then \codei{g} composed with \codei{f} is written \codei{g <> f} and is a program that, when applied to its input \codei{x}, computes \codei{g (f x)}. In Haskell, we can define function composition as:

\begin{code}
(.) :: (b -> c) -> (a -> b) -> a -> c
(.) g f x = g (f x)
\end{code}

\noindent where parentheses around the dot imply that function composition is an infix function.

While the size of the intermediate result of \codei{f} could potentially spoil any usefulness of the composition, functional programming solves this by only evaluating \codei{f} as much as is needed by \codei{g}. This property is referred to as lazy evaluation and lets us fuse functions without creating any unnecessary, intermediate values. Its benefits extend to embedded types as well, and guarantees fusion of vectors.

% The benefit of lazy evaluation is not limited to fusing functions and values, but extends to embedded languages as well. For instance, laziness in our co-design language ensures that only the parts of a program that contribute to the end result will be part of the function, that is, no unnecessary code will be generated for a program.

This section has given a brief overview of functional programming in Haskell and its beneficial properties for embedded languages. So far, the distinction between regular and embedded Haskell have yet been made. The following section introduces the concept of domain specific languages, and explains what it entails to be an embedded in Haskell. 

% The distinction between regular and embedded Haskell is given by the following section, and introduces the concept of domain specific languages in Haskell and what that entails.

% So far, a few Haskell functions has an introduction to functional programming and talked a bit about how its beneficial properties can help embedded languages. We have yet to make the distinction between regular and embedded Haskell. The following section introduces the concept of domain specific languages, and explains what it entails to be an embedded domain specific language in Haskell.

\section{Domain Specific Languages}
\label{domain}

A domain specific language (DSL) is a special-purpose language, tailored to a certain problem and captures the concepts and operations in its domain. For instance, a hardware designer might write in VHDL, while a web-designer that wants to create an interactive web-page would use JavaScript. DSLs come in two fundamentally different forms: external and internal, where VHDL and JavaScript are both examples of the former.

% An external DSL is a first-class language, with its own compiler or interpreter, and often comes with its own ecosystem.

% Both use a language that is specialized to the particular task they have at hand, and both build programs in a form that is familiar to regular programmers; VHDL and JavaScript are both DSLs.

Internal DSLs are embedded in a host language, and are often referred to as embedded domain specific languages (EDSLs). Haskell, with its static type system, flexible overloading and lazy semantics, has come to host a range of EDSLs~\cite{elliott2003}. For instance, popular libraries for parsing, pretty printing, hardware design and testing have all been embedded in Haskell~\cite{leijen2002, hughes1995, bjesse1998}.

EDSLs in Haskell are further divided into one of two kinds: shallow or deep. Conceptually, a shallow embedding captures the semantics of the data in a domain, whereas a deep embedding captures the semantics of the operations in a domain. Both kinds of embeddings have their own benefits and drawbacks. To illustrate the differences between shallow and deep embeddings we implement a small example domain:

\begin{code}
type Exp = Int

const :: Int -> Exp
const a = a

times :: Exp -> Exp -> Exp
times a b = a * b
\end{code}

\noindent where \codei{Exp} is a short-hand for expressions and is defined as a type synonym for integers thus an example of a shallow EDSL. Two functions are also provided: \codei{const} to lift integer literals, and \codei{times} to multiply expressions.

Two benefits of a shallowly embedded language like \codei{Exp} are that it is easy to add new functions and that evaluation is straightforward---the a value of type \codei{Exp} is the result of some expression. On the other hand, it is difficult to compile shallow types as there is no representation of the expressing that built its value. It is easier to compile an embedded language if its functions instead return an intermediate representation of their result, which sits between Haskell and the compiled code~\cite{elliott2003}. This technique is known as deep embedding, and \codei{Exp} can be reimplemented using it:

% The benefit of a shallowly embedded language like \codei{Exp} is that new functions can be added and evaluations is straightforward---evaluation is the identify function. A shallow type is however difficult to compile as its functions only return values, there is no representation of the computation leading up to that value. To compile an embedded language it is better for functions to return an intermediate representation of its result, which sits between Haskell and the compiled code~\cite{elliott2003}. This technique is known as a deep embedding and its functions return an abstract syntax tree that represents the computed value instead of its result. \codei{Exp} can be reimplemented to use a deep embedding:

\begin{code}
data Exp = Const Int | Times Exp Exp

const :: Int -> Exp
const a = Const a

times :: Exp -> Exp -> Exp
times a b = Times a b
\end{code}

\noindent where \codei{Exp} is now a datatype that lists all supported expressions, which \codei{const} and \codei{times} use to construct their results.

% \noindent Expressions now contain two constructors, one for integer literals and another for multiplication of expressions---Haskell's \codei{data} keyword introduces a new type and its different constructors are separated by a bar. These constructors forms what is often referred to as a syntax tree, and represent the computations behind an expression.

% \noindent \codei{const} and \codei{times} now return a representation of the result rather than the result itself. As a consequence, we cannot add new functions without first extending the \codei{Exp} type. From a user's perspective these functions are not that different from their shallow counterparts. In fact, an embedded language can have the look and feel of a stand-alone language.

% As \codei{const} and \codei{times} return a representation of their result rather than the result itself, it is harder to add new functions without first extending the \codei{Exp} datatype. The syntax tree used for a deeply embedded type like \codei{Exp}, while inflexible compared to its shallow version, enables interpretation of expressions in order to, for instance, evaluate them:

% As functions in a deeply embedded language return a representation of their result rather than the result itself, it is possible to build another function that interprets these to create, for instance, an evaluator:

As values in a deeply embedded language like \codei{Exp} are representations of the expressions that built it, rather than their result, it is possible to interpret them and, for example, define a function that evalutes them into integers:

\begin{code}
eval :: Exp -> Int
eval (Const a)   = a
eval (Times a b) = (eval a) * (eval b)
\end{code}

\noindent The ability to interpret values come at the cost of making it harder to add new functions over \codei{Exp} without first extending its datatype.

% \noindent Each line of \codei{eval} handles one of the two constructors in \codei{Exp}, translating each operation into its corresponding Haskell value. Evaluating an expression to its equivalent Haskell value is however not the only supported interpretation of expressions. We could just as well have compiled the same expression to, say, its corresponding C code.

While the implementation of shallow and deep embedding are usually at odds, there has been work done in order to combine their benefits~\cite{svenningsson2012}. The co-design language does make use of such a combination of deep and shallow embeddings: its core datatype is implemented using a deep embedding and user facing libraries use shallow embeddings built on top of the core. This mixture of embeddings ensures that the core is easy to interpret while simultaneously allowing user-facing libraries to provide a nice and extensible syntax.

% In the co-design language, we make use of such a combination of deep and shallow embeddings: the core syntax is implemented as a deep embedding, with user facing libraries as shallow embeddings on top. This mixture of embeddings means that our core language is easy to interpret, while the user-facing libraries are able to provide nice syntax for their functions.

At this point, this section and the previous one have given a brief overview of functional programming and domain specific languages, showcasing Haskell and the benefits its functional style provides for embedded languages. The next section introduces the co-design language through a few examples and highlights these benefits.

% Now that we have a grasp of functional programming, what domain specific languages in Haskell are and the ideas behind them, the next section will go through a larger example in order to showcase embedded programming in the co-design language.

\section{Embedded Programming in Haskell}
\label{embedded}

As we saw in sections~\ref{functional} and~\ref{domain}, programming in a functional language like Haskell is quite different from the imperative style of programming used in a language like C. As an example of these differences, and to showcase programming in a Haskell EDSL, we'll consider a finite impulse response (FIR) filter: one of the two primary types of digital filters used in digital signal processing applications~\cite{oppenheim1989}.

% In Haskell, users write their programs as a mathematical function from inputs to output, whereas in C they, users instead write their programs as a series of sequential steps to execute on some machine.

The mathematical definition of a FIR filter of rank $N$ is as follows:

\begin{equation}
y_{n} = b_{0} x_{n} + b_{1} x_{n-1} + \cdots + b_{N} x_{n-N} = \sum_{i=0}^{N} b_{i} x_{n-i}
\end{equation}
\vspace{1mm}

\noindent where $x$ and $y$ are the input and output signals, respectively, and $b_i$ is the value of the impulse response at time instant $i$. The inputs $x_{n-i}$ are sometimes referred to as ``taps'' as they tap into the input signal at various time instants. 

In an imperative language like C, we can implement the FIR filter as:

\begin{code}
void fir(int N, int L, double *b, double *x, double *y) {
 int j, k;
 double tap[256];
 for(j=0; j<N; j++) tap[j] = 0.0;
 for(j=0; j<L; j++) {
  for(k=N; k>1; k--) tap[k-1] = tap[k-2];
  tap[0] = x[j];
  y[j] = 0.0;
  for(k=0; k<N; k++) y[j] += b[k] * tap[k];
 }
}
\end{code}

\noindent Here, $N$ is the filter rank, as before, and $L$ is the size of the input---we assume $N$ will be smaller than $256$. The three variables $b$, $x$, and $y$ point towards arrays containing the coefficient, input, and output, respectively. As for the function body, the first for-loop initializes the taps while the second for-loop goes through all of the inputs and shifts them onto the taps and computes their impulse response.

At first glance, the C code seems to be a good representation of the FIR filter. There are however a few problems with the implementation. For instance, the last for-loops calculates a dot-product of the arrays $b$ and $tap$ ``inline'', as opposed to a stand-alone function which can be reused. While it is possible to extract the computation like so:

% Implementing a dot-product in this manner will tie it to our FIR filter...

\begin{code}
double dot(int N, double *xs, double *ys) {
  double sum = 0;
  for (int i=0; i<N; i++) sum += xs[i] * ys[i];
  return sum;
}
\end{code}

\noindent The function is still specialized to values of type $double$, it assumes $b$ and $tap$ both have at least $N$ elements, and it is not readily compositional since the function cannot be merged with the producers of \codei{xs} or \codei{ys} without looking at their implementation.

A dot product can be implemented in our embedded co-design language, as shown in section~\ref{intro}, using a similar imperative, but not idiomatic, style:

\begin{code}
dotSeq :: Arr Float -> Arr Float -> Program (Exp Float)
dotSeq x y = do
  sum <- initRef 0
  for 0 (min (length x) (length y) $ \ix -> do
    a <- getArr x ix
    b <- getArr y ix
    modifyRef sum $ \s -> s + a * b
  getRef sum
\end{code}

\noindent A program in the co-design languages is a monad: a kind of composable computation description. Together with Haskell's \codei{do} notation, a monad allows for instructions to be sequenced in much the same way as in C.

The above dot product is also not without fault. For instance, as with the C version, it is limited to values of type \codei{Float}; a dot product can be performed over any numerical value, as long as it supports addition and multiplication. Fortunately, functions over restricted types can be generalized to accept a range of values through Haskell's type classes. For numerical values, the type class \codei{Num} is used:

% , and we change the function's type to accept any value that satisfies \codei{Num} while leaving its body unchanged:

\begin{code}
dotSeq :: Num a => Arr a -> Arr a -> Program (Exp a)
\end{code}

\noindent \codei{dotSeq} is now polymorphic in the kind of values it accepts, but also limited to numerical. Even with its new type, the function is still quite fragile: the for-loop's length and the array indexing are both handled manually. That is, a single typo in any of these two would break the function but not its type, creating an error that would first emerge at run-time.

For purely array based computations like the dot product, the idiomatic approach would instead be to use our vector language:

% Where users can build larger functions from its library of smaller, pre-verified functions. A dot product can be defined as follows, using functions from this vector library:

\begin{code}
dotVec :: Num a => Vec a -> Vec a -> Exp a
dotVec xs ys = sum (zipWith (*) xs ys)
\end{code}

\noindent Here, the dot product is calculated by first joining the two vectors \codei{xs} and \codei{ys} by element-wise multiplication with \codei{zipWith}, and then reducing the resulting vector with \codei{sum}.

% Of these two functions, \codei{zipWith} is an example of a higher-order function, like the \codei{map} function from section~\ref{functional}; the manner in which it joins the two vectors is determined by the function given as its first argument.

A dot product based on vectors is not only closer to its mathematical specification than its sequential version, but also sturdier in the sense that it is harder for users to make an error: indices and lengths are now hidden by pre-verified vector functions. Furthermore, Haskell's lazy evaluation ensures that \codei{dotVec} can be merged freely with the producers of \codei{xs} and \codei{ys}: say, for example, that \codei{xs} is a constant vector of ones, \codei{zipWith} is then able to replace all indexing into \codei{xs} with a constant one.

As an example, we connect the dot product to a small program and compile it to C:

\begin{code}
int main() {
  uint16_t _a0[] = {1, 2, 3, 4}, *a0 = _a0;
  uint16_t _a1[] = {4, 3, 2, 1}, *a1 = _a1;
  uint16_t state2 = 0;
  uint32_t v3;
  for (v3 = 0; v3 < 4; v3++)
    state2 = a0[v3] * a1[v3] + state2;
  fprintf(stdout, "result: %d\n", state2);
  return 0;
}
\end{code}

\noindent The software program connects our dot product to two arrays of four unsigned integers and prints its result to standard output. Imports are omitted for brevity.

For a full implementation of the FIR filter, the vector language is a bit outside its comfort zone: vectors excel at describing array transformations, whereas the filter is described by a recurrence equation where output depends on previous input values. Nevertheless, the vector library does provide a few such recurrence functions, and we use one of them to implement the full filter:

\begin{code}
firVec :: Num a => Vec a -> Vec a -> Program (Arr a)
firVec cs v = recurrenceI (replicate (length cs) 0) v $ \i -> dotVec cs i
\end{code}

\noindent \codei{recurrenceI} takes an initial buffer, an input vector to iterate over, and a step function that produces one output at a time from the previous inputs and buffer. Note that \codei{reccurenceI} is not a combinator like the earlier vector functions, as it consumes the two input vectors to produce its output array. Different recurrence schemes would make use of other functions. 

% Recurrence functions can handle the regular feedback of the FIR filter quite well, but struggles for irregular access to earlier inputs. Nevertheless,

Compiling the \codei{firVec} to C yields the following code:

\begin{code}
int main() {    
  r5 = 0;
  for (v6 = 0; v6 <= 3; v6++) {
    a3[r5] = a1[v6];
    r5 = (r5 + 1) % 4;
    state7 = 0;
    for (v8 = 0; v8 < 4; v8++)
      state7 = a0[v8] * a3[(4 + r5 - v8 - 1) % 4] + state7;
    a2[v6] = state7;
  }
}
\end{code}

\noindent Imports, definitions, and code unrelated to the filter have been omitted for the sake of brevity. Note that \codei{a1} contains the input array and \codei{a0} the coefficients, both have a length of four, and array \codei{a3} is the queue used to store inputs. However, as all inputs present at once \codei{firVec}, it is possible to do without the queue:

% Seeing as how all the filters inputs is present at once in the input vector, we could double down on the vector based approach and do without the queue:

\begin{code}
firQ :: Num a => Vec a -> Vec a -> Vec a
firQ coeff = map (dotVec coeff . reverse) . tail . inits
\end{code}

\codei{firQ} trades the queue used by \codei{firVec}, and thus any modulo operations used during indexing, for a few stub-vectors that index directly into the input vector: \codei{inits} creates a vector from each initial segment of the input and \codei{tail} drops the first of these vectors. Compilation yields the following C code:

\begin{code}
int main() {
  for (v3 = 0; v3 <= 3; v3++) {
    if (4 <= v3 + 1) { b5 = 4; } else { b5 = v3 + 1; }
    if (4 <= b5) { b4 = 4; }
    else {
      if (4 <= v3 + 1) { b6 = 4; } else { b6 = v3 + 1; }
      b4 = b6;
    }
    state7 = 0;
    for (v8 = 0; v8 < b4; v8++) {
      if (4 <= v3 + 1) { b9 = 4; } else { b9 = v3 + 1; }
      state7 = a0[v8] * a1[b9 - v8 - 1] + state7;
    }
    a2[v3] = state7;
  }
}
\end{code}

While a FIR filter can be described using vectors, an idiomatic approach for such functions is to instead use our signal processing language. The language is built upon the co-design language and introduces the concept of signals: possibly infinite sequences of values. Like the vector language, idiomatic signal functions are constructed compositionally using smaller functions, but signals also provide a function for introducing unit delays.

As an example, we create three signal functions that represent the main components of a FIR filter:

\begin{code}
sums :: Num a => [Sig a] -> Sig a
sums as = foldr1 (+) as

muls :: Num a => [Exp a] -> [Sig a] -> [Sig a]
muls as bs = zipWith (*) (map constant as) bs

dels :: Exp a -> Sig a -> [Sig a]
dels e as = iterate (delay e) as
\end{code}

\noindent That is, a summation and a multiplication with coefficients, which together form a dot product, and a number of successive delays to form the taps.

We should note that \codei{foldr1}, \codei{zipWith}, \codei{map} and \codei{iterate} are the standard Haskell functions for lists, as opposed to signal functions. Addition and multiplication are lifted to operate element-wise over signals. \codei{constant} and \codei{delay} are proper signal functions and introduce a constant signal and a unit delay, respectively.

A full FIR filter can now be expressed with signals as:

\begin{code}
firSig :: Num a => [Exp a] -> Sig a -> Sig a
firSig coeffs = sums . muls coeffs . dels 0
\end{code}

\noindent which also is quite close to the filter's mathematical specification. Especially so from a hardware designers perspective: the input signal is delayed to form the filter's taps, where each ``tap'' is multiplied with a coefficient and summed. Hardware designers tend to be comfortable with this kind of box design, where each box corresponds to a function.

Compiling \codei{firSig} to VHDL produces the following hardware design:

% TODO: Might be an error in the code, as both processes trigger on the clock.
\begin{code}
ENTITY comp0 IS
  PORT (in0 : IN unsigned (7 DOWNTO 0);
        out1 : OUT unsigned (7 DOWNTO 0);
        clk : IN std_logic;
        rst : IN std_logic) ;
END ENTITY comp0 ;
ARCHITECTURE behav OF comp0 IS
  SIGNAL state2 : unsigned (7 DOWNTO 0) ;
  SIGNAL state2_d : unsigned (7 DOWNTO 0) := "00000000" ;
BEGIN
  l8 :
    PROCESS (in0) IS
      VARIABLE v3, v4, v5, v6, v7 : unsigned (7 DOWNTO 0) ; 
    BEGIN
      v3 := "00000001" ;
      v4 := "00000010" ;
      v5 := resize (v3 * ins0, 8) ;
      v6 := resize (v4 * state2_d, 8) ;
      v7 := resize (v5 + v6, 8) ;
      state2 <= in0 ;
      out1 <= v7 ;
    END PROCESS l8 ;
  l9 :
    PROCESS (clk) IS
    BEGIN
      IF rising_edge (clk) THEN
        state2_d <= state2 ;
      END IF ;
    END PROCESS l9 ;
END ARCHITECTURE behav ;
\end{code}

\section{Summary}

Section~\ref{intro} gave a general introduction to embedded programming and its challenges. Specifically, the problem of extracting performance from an embedded system. Heterogeneous system is then introduced as a possible response with potential, where section~\ref{background} mentioned modern FPGAs in particular as a heterogeneous system of interest.

Heterogeneous systems are not without their own challenges, as the presence of multiple processors raises all of the issues involved with parallel, homogeneous systems. Also, the level of heterogeneity in a system can introduce additional challenges with different system capabilities and development between processors: components may support different instructions, leading to incompatibilities between the code they can execute even if they're both programmed in the same language.

Functional languages were then introduced in section~\ref{functional} as a solution to the various modularity issues with using lower-level languages like C or VHDL for heterogeneous systems. Particularly the ``glue code'' of functional languages, that is their higher-order functions, type-system, and lazy evaluation, was shown to be useful for developing reusable components. Section~\ref{domain} showed how to embedded a language in Haskell, a functional programming language, and how these embedded languages benefit from the aforementioned benefits as well.

Section~\ref{embedded} went on to introduce our current attempt at bringing the benefits of functional programming languages to the domain of embedded heterogeneous systems with our hardware software co-design, vector and signal languages. The aim is to have the co-design language serve as a convenient description of imperative program, both software and hardware, with support for compilation to C and VHDL. On the other hand, our vector and signal language will serve as convenient front-ends for the co-design language, extending it with support for array and synchronous data-flow programming, respectively.

The remainder of this thesis goes over the languages in detail to highlight the various ideas they are built on. In particular, the following contributions are made:

\begin{itemize}
\item We present a language for hardware software co-design that is embedded in Haskell and designed with FPGA programming in mind. As such, the co-design language generates both C and VHDL code to describe its software and hardware components, including the necessary glue code for connections between components. Intrinsics of a component do however vary between components, even those described by the same language. Both our software and hardware languages are therefore extensible so that they may be of use in other systems as well.

\item We present two extensions to the hardware software co-design language. One extension supplements an array type with vector computations that can be defined in a compositional manner that supports fusion. The other extension supplements an expression type with synchronous data-flow and provides its own interpreter for turning such expressions into signal processing networks. Neither extension is dependent on the co-design language and can be fitted for use in other embedded languages.

\item We present the type-based techniques for implementing an embedded language like the co-design language. The technique is based on a monadic representation of imperative programs that is loosely coupled to its expression and predicate types, allowing each part to be developed separately. As an additional benefit, handling the sequencing of programs separately enables their interpretation to be handled separately as well. Programs and their interpretation are both designed with extensibility in mind.

\item We present a code generation scheme for programs by a series of translations between progressively smaller languages, where each step is typed in order to safeguard against common errors found in untyped translations. This scheme allows for languages to provide users with feature-rich expressions, while still having fine control over the generated source code.
\end{itemize}

\end{document}

%%  LocalWords:  DSLs EDSLs
