%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .--~*teu.
%  dF     988Nx
% d888b   `8888>
% ?8888>  98888F
%  "**"  x88888~
%       d8888*`
%     z8**"`   :
%   :?.....  ..F
%  <""888888888~
%  8:  "888888*
%  ""    "**"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[../paper.tex]{subfiles}
\begin{document}

\chapter{Background}
\label{background}

High demands for efficiency under resource constraints have greatly influenced the development of embedded systems, both in terms of programming practice and architecture. Today, we see embedded systems consisting of everything between general purpose processors (GPPs) and application specific integrated circuits (ASICs).

GPPs and ASICs represent two extremes of available architectures, where field programmable gate arrays (FPGAs) have found a good middle-ground and provide the best of both worlds: they are close to hardware and can be reprogrammed~\cite{bacon2013}. Modern FPGAs also contain various discrete components and co-processors which, together with their good performance per Watt ratio, have seen them increasingly used in high-performance, computationally intensive systems~\cite{mcmillan2014}.

A modern FPGA show great promise as a prototypical system for heterogeneous computing, but their adoption has been slowed by the fact that they're difficult to program. The logic blocks of an FPGA are usually programmed in a hardware description language, while the embedded processors are programmed in a low level dialect of C or even assembler. Furthermore, the various components of a modern FPGA may support different intrinsics and therefore have incompatibilities between the code they can execute even if they are programmed in the same language.

One of the benefits of using low level languages for embedded systems is that they give a designer fine control over a system capabilities. However, this control comes at a cost, as the programmers must exercise this right during the entire design process. So the problem of implementing an algorithm has become a problem of implementing an algorithm for a specific component.

The issues with low level languages are magnified for heterogeneous systems, as the developer must specify both its hardware and software parts and how they communicate; ideally she would like to experiment with various choices of what to put in hardware and what in software. Low-level languages provide little support for such design exploration, and rewriting code intended for one component to another is typically a major undertaking.

Many of the issues faced in heterogeneous computing with low-level languages stem from a lack of abstractions. Some of these languages' modularity problems come as a direct result of the fine grained control they provide, as it will inevitably tie programs to the architecture of its system. Other issues of, for instance, functionality and architecture come as a indirect consequence of the lack of abstractions. Ideally, such issues would be treated separately, as they allow for the creation of small, reusable libraries that provide solutions to these issues, and then combined in a modular fashion to solve larger problems.

In the paper ``Why functional programming matters''~\cite{hughes1989}, Hughes argues that many of the above modularity problems can be address by making use of functional programming. Particularly the glue code that functional programming languages offer, through higher-order functions and lazy evaluation, enables us to build useful combinators.

The benefits of a functional programming language is however not limited to software development, as Sheeran shows in her paper ``Hardware Design and Functional Programming: a Perfect Match''~\cite{sheeran2005}. Where she exemplifies how a functional language can make it easy to explore and analyze hardware designs in a way that traditional hardware description languages would have found difficult, if not impossible.

Before we go into combining these benefits for heterogeneous computing, an introduction to functional programming and embedded languages is in order.

\section{Functional Programming}
\label{functional}

Functional programming, as the name implies, is based around the application of a function to its arguments. In this programming style, a program is written as a function that accept input and deliver its result. This function itself is defined in terms of smaller functions, which in turn are defined using smaller functions still and, in the end, a function consists of nothing but language primitives.

An important distinction between functions in a functional programming language and, say an imperative language like C, is that functions always return the same value when given same arguments. More generally, we say that functional programs have no side effects; functions can safely be evaluated in parallel as long at their data dependencies are satisfied.

A function that accepts other functions as arguments is often referred to as a higher-order function, or a combinator, and provides a useful piece of glue code that lets programmers build complex functions from smaller ones. In Haskell, a functional programming language, a number of such higher-order functions that implement common operations are provided by its standard libraries. One such function is \codei{map} and can be defined as follows:

\begin{code}
map :: (a -> b) -> [a] -> [b]
map f []     = []
map f (x:xs) = f x : map f xs
\end{code}

The first line specifies the type of \codei{map}, because in Haskell, every function is assigned a static type in an effort to attain safer programs. If you by mistake write a program that tries to multiply some integer by a boolean type, it won't even compile and instead warns you about the type error. As for the types themselves, they are a kind of label that every expression has and states what category of operations that the expression belongs in.

A function's type comes after the \codei{::} sign, and in the case of \codei{map}, tells us that its first argument is a function \codei{f :: a -> b} which, given an argument of type \codei{a}, produces a result of type \codei{b}. In addition to the function, \codei{map} also takes a list \codei{xs :: [a]} of element with type \codei{a}, and returns another list of elements with type \codei{b}. As functions cannot have any side effects, we can already make a guess at what this function does and claim that it applies \codei{f} to every element of \codei{xs}.

The second and third line of \codei{map} validates our earlier guess and lists the full definition of the function. Firstly, it says that given an empty list, shown as \codei{[]}, the result is another empty list---there's simply nothing to apply \codei{f} to. Secondly, in the case where \codei{map} is given a non-empty list \codei{x:xs} where \codei{x} is the list head and \codei{xs} its tail, it applies \codei{f} to \codei{x} and concatenates its result with the list made from a recursive call to itself on \codei{xs}.

The usefulness of higher-order functions like \codei{map} comes from their ability to encode common patterns: \codei{map} works for all functions and lists that fit its type signature. Functions like \codei{map} are often referred to as combinators, a style of organizing libraries around a few primitives values and functions for combining them. These combinators allow for complex structures to be built with a small set of verified combinators. For example, the earlier dot-product from section~\ref{intro} is composed of two such combinators: \codei{zipWith}, a generic way of joining two vectors, and \codei{sum}.

The other piece of glue code that functional programming languages provides is often referred to as function composition, and enables programs to be glued together. Say that \codei{f} and \codei{g} are two programs, then \codei{g} composed with \codei{f} is written \codei{g <> f} and is a program that, when applied to its input \codei{x}, computes \codei{g (f x)}. In Haskell, we can define function composition as:

\begin{code}
(.) :: (b -> c) -> (a -> b) -> a -> c
(.) g f x = g (f x)
\end{code}

\noindent where parenthesis around the dot implies that function composition is an infix function. While the size of the intermediate result of \codei{f} could spoil the usefulness of composition, functional programming solves this by only evaluating \codei{f} as much as is needed by \codei{g}. This property is referred to as lazy evaluation.

The benefits of lazy evaluation is not limited to fusing functions and values, but extends to embedded languages as well. For instance, laziness in our co-design language ensures that only the parts of a program that contribute to the end result will be part of the function, that is, no unnecessary code will generated for a program. % Also, laziness is especially useful for vectors as it can guarantee fusion.

So far, we have looked at a few Haskell functions as an introduction to functional programming and talked a bit about how its beneficial properties can help embedded languages. We have yet to make the distinction between regular and embedded Haskell. The following section introduces the concept of domain specific languages, and explains what explains what it entails to be an embedded domain specific language in Haskell.

\section{Domain Specific Languages}
\label{domain}

A domain specific language (DSL) is a special-purpose language, tailored to a certain problem and captures the concepts and operations in its domain. For instance, a hardware designer might write in VHDL, while a web-designer that wants to create an interactive web-page would use JavaScript. Both use a language that is specialized to the particular task they have at hand, and both build programs in a form that is familiar to regular programmers; VHDL and JavaScript are both examples of a DSL.

Haskell, with its static type system, flexible overloading and lazy semantics, has come to host a range of EDSLs~\cite{elliott2003}. For instance, popular libraries for parsing, pretty printing, hardware design and testing have all been embedded in Haskell~\cite{leijen2002, hughes1995, bjesse1998}. These DSLs come in two fundamentally different forms: external and internal. An external DSL is a first-class language, with its own compiler or interpreter, and often comes with its own ecosystem. Internal DSLs are embedded in a host language, and are often referred to as embedded domain specific languages (EDSLs). The co-design, vector and signal languages presented in this thesis are all examples of internal DSLs.

We illustrate how shallow embeddings work in Haskell through a small example:

\begin{code}
type Exp = Int

const :: Int -> Exp
const a = a

times :: Exp -> Exp -> Exp
times a b = a * b
\end{code}

\noindent \codei{Exp} is a short-hand for expressions and is defined as a type synonym for integers, making use of Haskell's \codei{type} keyword. The language around our expressions is defined by two functions, one for integer literals, called \codei{const} and another for multiplication, called \codei{times}. Note we could easily have added more functions, as long as they're integral expressions.

Another advantage of \codei{Exp} is that we can quickly calculate its value:

\begin{code}
eval :: Exp -> Int
eval a = a
\end{code}

\noindent Shallowly embedded types do however perform quite poorly if we wish to compile the language; functions only return values and provide no way to look at the representation of a program. To compile an embedded language it is better to use an intermediate representation for functions, which sits between Haskell and the compiled code. This technique is known as a deep embedding and its functions return an abstract syntax tree that represents the computed value instead of its result.

We can reimplement our earlier \codei{Exp} type using deep embedding:

\begin{code}
data Exp = Const Int | Times Exp Exp
\end{code}

\noindent Expressions now contain two constructors, one for integer literals and another for multiplication of expressions---Haskell's \codei{data} keyword introduces a new type and its different constructors are separated by a pipe. These constructors forms what is often referred to as a syntax tree, and represents the computations behind an expression.

Having changed \codei{Exp} we must also change the functions that comes with it:

\begin{code}
const :: Int -> Exp
const a = Const a

times :: Exp -> Exp -> Exp
times a b = Times a b
\end{code}

\noindent \codei{const} and \codei{times} now returns a representation of the result rather than the result itself. As a consequence, we cannot add new functions without first extending the \codei{Exp} type. From a user's perspective these functions are not that different from their shallow counterparts. In fact, embedded languages can have the look and feel of a stand-alone language. %, since they reuse many parts of the host-language's ecosystem and semantics.

The syntax tree used for a deeply embedded type like \codei{Exp}, while inflexible compared to its shallow version, is what enables functions to inspect, modify, and interpret an expression in order to support, for example, their evaluation:

\begin{code}
eval :: Exp -> Int
eval (Const a)   = a
eval (Times a b) = (eval a) * (eval b)
\end{code}

\noindent Each line of \codei{eval} handles one of the two constructors in \codei{Exp}, translating them into their corresponding Haskell value. Evaluating an expression to its equivalent Haskell value is however not the only supported interpretation of expressions. We could just as well have compiled the same expression to, say, its corresponding C code.

While the implementation of shallow and deep embedding are usually at odds, there has been work done in order to combine their benefits~\cite{svenningsson2012}. In our co-design language, we make use of such a combination of deep and shallow embeddings: the core syntax is implemented as a deep embedding, with user facing libraries as shallow embeddings on top. This mixture of embeddings means that our core language is easy to interpret, while the user-facing libraries are able to provide a nice syntax for their functions.

Now that we have grasp of functional programming, what domain specific languages in Haskell are and the ideas behind them, the next section will go through a larger example in order to showcase embedded programming in the co-design language.

\section{Embedded Programming in Haskell}
\label{embedded}

As we saw in the section~\ref{functional} and~\ref{domain}, programming in a functional language like Haskell is widely different from the imperative style used in a language like C. In Haskell, users write their programs as a mathematical function from inputs to output, while in C they write them as a series of sequential steps to execute on some machine.

As an example of the above differences, we'll consider a finite impulse response (FIR) filter, one of the two primary types of digital filters used in digital signal processing applications~\cite{oppenheim1989}. The mathematical definition of a FIR filter of rank $N$ is as follows:

\begin{equation}
y_{n} = b_{0} x_{n} + b_{1} x_{n-1} + \cdots + b_{N} x_{n-N} = \sum_{i=0}^{N} b_{i} x_{n-i}
\end{equation}
\vspace{1mm}

\noindent where $x$ and $y$ are the input and output signals, respectively, and $b_i$ is the value of the impulse response at time instant $i$. The inputs $x_{n-i}$ are sometimes referred to as ``taps'' as they tap into the input signal at various time instants. 

In an imperative language like C, we can implement the FIR filter as:

\begin{code}
void fir(int N, int L, double *b, double *x, double *y) {
 int j, k;
 double tap[256];
 for(j=0; j<N; j++) tap[j] = 0.0;
 for(j=0; j<L; j++) {
  for(k=N; k>1; k--) tap[k-1] = tap[k-2];
  tap[0] = x[j];
  y[j] = 0.0;
  for(k=0; k<N; k++) y[j] += b[k] * tap[k];
 }
}
\end{code}

\noindent Here, $N$ is the filter rank, as before, and $L$ is the size of the input---we assume $N$ will be smaller than $256$. The three variables $b$, $x$, and $y$ point towards arrays containing the coefficient, input, and output, respectively. As for the functions body, the first for-loop initializes the taps while the second for-loop goes through all of the inputs and shifts them onto the taps and computes their impulse response.

At first glance the C code seems to be a good representation of the FIR filter. There are however a few problems with the implementation. For instance, the second of the two inner for-loops that calculates the intermediate result $v$ does so by performing a dot-product of the arrays $b$ and $tap$. Implementing a dot-product in this manner will tie it to our FIR filter, as opposed to a stand-alone function which can be used by many. While it is possible to extract the computation like so:

\begin{code}
double dot(int N, double *xs, double *ys) {
  double sum = 0;
  for (int i=0; i<N; i++) sum += xs[i] * ys[i];
  return sum;
}
\end{code}

\noindent The function is still specialized to values of type $double$, it assumes $b$ and $tap$ both have at least $N$ elements, and it is not readily compositional since the function cannot be merged with the producers of \codei{xs} or \codei{ys} without looking at its implementation.

A dot product can be implemented in our embedded co-design language, as shown in section~\ref{intro}, using a similar imperative, but not idiomatic, style:

\begin{code}
dotSeq :: Arr Float -> Arr Float -> Program (Exp Float)
dotSeq x y = do
  sum <- initRef 0
  for 0 (min (length x) (length y) $ \ix -> do
    a <- getArr x ix
    b <- getArr y ix
    modifyRef sum $ \s -> s + a * b
  getRef sum
\end{code}

\noindent That is, a program in the co-design languages is a monad. These monads acts as a kind of composable computation descriptions, and together with Haskell's \codei{do} notation, allows for instructions to be sequenced very much like they are in C.

This version of a dot product is however not without fault. As with the C version, the function is locked to values of \codei{Float}. We could however tackle this problem by making use of Haskell's \codei{Num} class, changing the function's type while leaving its body intact:

\begin{code}
dotSeq :: Num a => Arr a -> Arr a -> Program (Exp a)
\end{code}

The dot-product is now polymorphic in the kind of values it accepts, but limited to numerical values that supports addition and multiplication. Even with its new type, the function is quite still fragile: the for-loop's length and the array indexing are both handled manually. That is, a single typo in any of these two would break the function but not its type, creating an error that would first emerge at run-time.

For purely array based computations like the dot product, the idiomatic approach would instead be to use our vector language. Where users can build larger functions from its library of smaller, pre-verified functions. A vector version can be defined as follows:

\begin{code}
dotVec :: Num a => Vec a -> Vec a -> Exp a
dotVec xs ys = sum (zipWith (*) xs ys)
\end{code}

\noindent Here, the dot product is calculated by first joining the two vectors \codei{xs} and \codei{ys} by element-wise multiplication with \codei{zipWith}, and then reducing the resulting list with \codei{sum}.

The above version of the dot product is not only closer to its mathematical specification than the sequential one, but also sturdier in the sense that its harder for users to make an error. Furthermore, Haskell's lazy evaluation ensures that \codei{dotVec} can be merged freely with the producers of \codei{xs} and \codei{ys}.

For a full implementation of the FIR filter, the vector language is a bit outside its comfort zone. Vectors excel at describing array transformations, whereas the filter is described by a recurrence equation where output depends on previous input values. Nevertheless, the vector library does provide a few such recurrence functions, and we use one of them to implement the full filter:

\begin{code}
firVec :: Num a => Vec a -> Vec a -> Program (Arr a)
firVec cs v = recurrenceI (replicate (length bs) 0) v $ \i -> dotVec cs i
\end{code}

\noindent Where \codei{recurrenceI} takes an initial buffer, an input vector to iterate over, and a step function that produces one output at a time from the previous inputs and buffer. Recurrence functions can handle the regular feedback of a FIR filter quite well, but struggles for irregular access to earlier inputs.

% Locking the type of \codei{firVec} to, for example, 32-bit signed integers, we compile it to C:

% \begin{code}
% ghci> icompile (firVec (-1...1) (1...10) :: Program (Arr Int32))
% Generated C code goes here.
% \end{code}

While a FIR filter can be described using vectors they are typically used in digital signal processing applications, and idiomatic approach for such functions is to instead use our signal processing language.  The language is built upon the co-design language and introduces the concept of signals: possibly infinite sequences of values. Like the vector language, idiomatic signal functions are constructed compositionally using smaller functions, but signals also provide a function for introducing unit delays. As an example, we create three signal functions that represents the main components of a FIR filter:

\begin{code}
sums :: Num a => [Sig a] -> Sig a
sums as = foldr1 (+) as

muls :: Num a => [Exp a] -> [Sig a] -> [Sig a]
muls as bs = zipWith (*) (map constant as) bs

dels :: Exp a -> Sig a -> [Sig a]
dels e as = iterate (delay e) as
\end{code}

\noindent That is, a summation and a multiplication with coefficients, which together form a dot product, and a number of successive delays to form the taps.

We should note that \codei{foldr1}, \codei{zipWith}, \codei{map} and \codei{iterate} are the standard Haskell functions for lists. Whereas addition and multiplication are lifted to operate element-wise over signals. The other two functions, \codei{constant} and \codei{delay}, are signal functions that introduces a constant signal and a unit delay, respectively.

A full FIR filter can now be expressed as:

\begin{code}
firSig :: Num a => [Exp a] -> Sig a -> Sig a
firSig coeffs = sums . muls coeffs . dels 0
\end{code}

\noindent Which is quite close to the filter's mathematical specification: the input signal is delayed to form the filter's taps, where each ``tap'' is multiplied with a coefficient and summed.

% Locking the type signature of \codei{firSig} to signals over 32-bit integers we compile it to C:

% \begin{code}
% ghci> icompileFun (firSig [-1,0,1] :: Sig Int32 -> Sig Int32)
% Generated C code goes here.
% \end{code}

\section{Summary}

Section~\ref{intro} gave a general introduction to embedded programming and its challenges. Specifically the problem of extracting performance from an embedded system while keeping its power cost down was discussed. Heterogeneous systems was then introduced as a possible response to these problems, where section~\ref{background} mentioned modern FPGAs in particular as a heterogeneous system of interest.

Heterogeneous systems are not without their own challenges, as the presence of multiple processors raises all of the issues involved with parallel, homogeneous systems. Also, the level of heterogeneity in a system can introduce additional challenges with different system capabilities and development between processors: components may support different instructions, leading to incompatibilities between the code they can execute even if they're both programmed in the same language.

Functional languages was then introduced in section~\ref{functional} as a response to the various modularity issues with using lower-level languages like C or VHDL for heterogeneous systems. Particularly the ``glue code'' of functional languages, that is their higher-order functions, type-system, and lazy evaluation, was shown to be useful for developing reusable components. Section~\ref{domain} showed how these benefits extended to languages embedded in a functional language like Haskell.

Section~\ref{embedded} went on to introduce our current attempt at bringing the benefits of functional programming languages to the domain of embedded heterogeneous systems with our hardware software co-design, vector and signal languages. The aim is to have the co-design language serve as a convenient description of imperative program descriptions for both software and hardware, with support for compilation to C and VHDL. Both the software and hardware parts are extensible to account for any differences in the intrinsics of components, and in terms of possible interpretations. The vector and signal languages are both built upon the co-design language. The first serves as a convenient front-end for array based programs, while the other extends it with support for expressing synchronous data-flow computations.

\end{document}
