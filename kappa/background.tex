%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .--~*teu.
%  dF     988Nx
% d888b   `8888>
% ?8888>  98888F
%  "**"  x88888~
%       d8888*`
%     z8**"`   :
%   :?.....  ..F
%  <""888888888~
%  8:  "888888*
%  ""    "**"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[../paper.tex]{subfiles}
\begin{document}

\chapter{Background}
\label{background}

Today, we see embedded systems consisting of everything between general purpose processors (GPPs) and application specific integrated circuits (ASICs). GPPs and ASICs represent two extremes of available architectures, where field programmable gate arrays (FPGAs) have found a good middle-ground and provide the best of both worlds: they are close to hardware and can be reprogrammed~\cite{bacon2013}. Modern FPGAs also contain various discrete components and co-processors which, together with their good performance per Watt ratio, have seen them increasingly used in high-performance, computationally intensive systems~\cite{mcmillan2014}. While a modern FPGA shows great promise as a prototypical system for heterogeneous computing, its adoption has been slowed by the fact that it is difficult to program.

The logic blocks of an FPGA are usually programmed in a hardware description language, while its co-processors are programmed in some low level dialect of C or even assembler. Low level languages are typically used for embedded systems as they give designers fine control over the system's capabilities. Such fine control does however come at a cost, as the programmers must exercise this right during the entire design process. So the problem of implementing an algorithm has become a problem of implementing an algorithm for a specific system's architecture.

The issues with low level languages are magnified for heterogeneous systems, as the developer must specify both its hardware and software parts and how they communicate; ideally she would like to experiment with various choices of what to put in hardware and what in software. Low-level languages provide little support for such design exploration, and rewriting code intended for one of the FPGA's processing elements to another is typically a major undertaking.

Many of the issues faced in heterogeneous computing with low-level languages stem from a lack of abstractions. Some of these languages' modularity problems come as a direct result of the fine grained control they provide. Other issues of, for instance, functionality and architecture come as an indirect consequence of the lack of abstractions. Ideally, such issues would be treated separately from an algorithm's design.

In the paper ``Why functional programming matters''~\cite{hughes1989}, Hughes argues that many of the above modularity problems can be addressed by making use of functional programming. Particularly the glue code that functional programming languages offer, through higher-order functions and lazy evaluation, enables us to build useful combinators.

The benefits of a functional programming language are however not limited to software development, as Sheeran shows in her paper ``Hardware Design and Functional Programming: a Perfect Match''~\cite{sheeran2005}. Sheeran exemplifies how a functional language can make it easy to explore and analyze hardware designs in a way that traditional hardware description languages would have found difficult, if not impossible.

Before we go into combining these benefits for heterogeneous computing, an introduction to functional programming and embedded languages is in order.

\section{Functional Programming}
\label{functional}

Functional programming is based around the application of a function to its arguments. In this programming style, a program is written as a function that accepts input and delivers its result. That function itself is defined in terms of smaller functions, which in turn are defined using smaller functions still and, in the end, a function consists of nothing but language primitives.

An important distinction between functions in a functional programming language and, say, an imperative language like C, is that functions always return the same value when given the same arguments. More generally, we say that functional programs have no side effects: functions can safely be evaluated in parallel as long as their data dependencies are satisfied.

A function that accepts other functions as arguments is often referred to as a higher-order function, or a combinator, and provides a useful piece of glue code that lets programmers build complex functions from smaller ones. In Haskell, a number of such higher-order functions are provided by its standard libraries. One such function is \codei{map} and can be defined as follows:

\begin{code}
map :: (a -> b) -> [a] -> [b]
map f []     = []
map f (x:xs) = f x : map f xs
\end{code}

The first line specifies the type of \codei{map}, because in Haskell, every function is assigned a static type in an effort to attain safer programs---if a function tries to multiply an integer with a boolean, the compiler will reject the function and instead point out the type mismatch. In the case of \codei{map}, its type is a function from another function \codei{f :: a -> b} and a list \codei{xs :: [a]} to an element of type \codei{b}. The second and third line of \codei{map} specifies that it, when given an empty list as denoted by \codei{[]}, returns another empty list, and for non-empty lists, applies \codei{f} to the list's head and recursively calls itself on the list's tail.

The usefulness of higher-order functions like \codei{map} comes from their ability to encode common patterns: \codei{map} works for all functions and lists that fit its type signature. Functions like \codei{map} are often referred to as combinators, a style of organizing libraries around a few primitive values and functions for combining them. These combinators allow for complex structures to be built from a small set of pre-verified functions. For example, the earlier dot-product from section~\ref{intro} is composed of two combinators: \codei{zipWith}, a generic way of joining two vectors, and \codei{sum}:

\begin{code}
zipWith :: (a -> b -> c) -> Vec a -> Vec b -> Vec c
zipWith f a b = fmap (uncurry f) $ zip a b

sum :: Vec Int -> Int
sum = fold (+) 0
\end{code}

\noindent \codei{fmap} here is similar to the above \codei{map} but works for vectors instead of lists, \codei{zip} joins two vectors into a single vector of pairs, and \codei{fold} reduces a vector into a scalar value using addition and starting at zero.

The other piece of glue code that functional programming languages provides is often referred to as function composition, and enables programs to be glued together. Say that \codei{f} and \codei{g} are two programs, then \codei{g} composed with \codei{f} is written \codei{g <> f} and is a program that, when applied to its input \codei{x}, computes \codei{g (f x)}. In Haskell, we can define function composition as:

\begin{code}
(.) :: (b -> c) -> (a -> b) -> a -> c
(.) g f x = g (f x)
\end{code}

\noindent where parentheses around the dot imply that function composition is an infix function.

While the size of the intermediate result of \codei{f} could potentially spoil any usefulness of the composition, functional programming solves this by only evaluating \codei{f} as much as is needed by \codei{g}. This property is referred to as lazy evaluation and lets us fuse functions without creating any unnecessary, intermediate values. Its benefits extend to embedded types as well, and guarantees fusion of vectors.

This section has given a brief overview of functional programming in Haskell and its beneficial properties for embedded languages. So far, the distinction between regular and embedded Haskell have yet been made. The following section introduces the concept of domain specific languages, and explains what it entails to be an embedded in Haskell. 

\section{Domain Specific Languages}
\label{domain}

A domain specific language (DSL) is a special-purpose language, tailored to a certain problem and captures the concepts and operations in its domain. For instance, a hardware designer might write in VHDL, while a web-designer that wants to create an interactive web-page would use JavaScript. DSLs come in two fundamentally different forms: external and internal, where VHDL and JavaScript are both examples of the former.

Internal DSLs are embedded in a host language, and are often referred to as embedded domain specific languages (EDSLs). Haskell, with its static type system, flexible overloading and lazy semantics, has come to host a range of EDSLs~\cite{elliott2003}. For instance, popular libraries for parsing, pretty printing, hardware design and testing have all been embedded in Haskell~\cite{leijen2002, hughes1995, bjesse1998}.

EDSLs in Haskell are further divided into one of two kinds: shallow or deep. Conceptually, a shallow embedding captures the semantics of the data in a domain, whereas a deep embedding captures the semantics of the operations in a domain. Both kinds of embeddings have their own benefits and drawbacks. To illustrate the differences between shallow and deep embeddings we implement a small example domain:

\begin{code}
type Exp = Int

const :: Int -> Exp
const a = a

times :: Exp -> Exp -> Exp
times a b = a * b
\end{code}

\noindent where \codei{Exp} is a short-hand for expressions and is defined as a type synonym for integers thus an example of a shallow EDSL. Two functions are also provided: \codei{const} to lift integer literals, and \codei{times} to multiply expressions.

Two benefits of a shallowly embedded language like \codei{Exp} are that it is easy to add new functions and that evaluation is straightforward---the a value of type \codei{Exp} is the result of some expression. On the other hand, it is difficult to compile shallow types as there is no representation of the expressing that built its value. It is easier to compile an embedded language if its functions instead return an intermediate representation of their result, which sits between Haskell and the compiled code~\cite{elliott2003}. This technique is known as deep embedding, and \codei{Exp} can be reimplemented using it:

\begin{code}
data Exp = Const Int | Times Exp Exp

const :: Int -> Exp
const a = Const a

times :: Exp -> Exp -> Exp
times a b = Times a b
\end{code}

\noindent where \codei{Exp} is now a datatype that lists all supported expressions, which \codei{const} and \codei{times} use to construct their results.

As values in a deeply embedded language like \codei{Exp} are representations of the expressions that built it, rather than their result, it is possible to interpret them and, for example, define a function that evalutes them into integers:

\begin{code}
eval :: Exp -> Int
eval (Const a)   = a
eval (Times a b) = (eval a) * (eval b)
\end{code}

\noindent The ability to interpret values come at the cost of making it harder to add new functions over \codei{Exp} without first extending its datatype.

While the implementation of shallow and deep embedding are usually at odds, there has been work done in order to combine their benefits~\cite{svenningsson2012}. The co-design language does make use of such a combination of deep and shallow embeddings: its core datatype is implemented using a deep embedding and user facing libraries use shallow embeddings built on top of the core. This mixture of embeddings ensures that the core is easy to interpret while simultaneously allowing user-facing libraries to provide a nice and extensible syntax.

At this point, this section and the previous one have given a brief overview of functional programming and domain specific languages, showcasing Haskell and the benefits its functional style provides for embedded languages. The next section introduces the co-design language through a few examples and highlights these benefits.

\section{Embedded Programming in Haskell}
\label{embedded}

Programming in a functional language like Haskell is quite different from the imperative style of programming used in a language like C. As an example of these differences, consider a finite impulse response (FIR) filter, one of the two primary types of digital filters used in digital signal processing applications~\cite{oppenheim1989}. The mathematical definition of a FIR filter of rank $N$ is as follows:

\begin{equation}
y_{n} = b_{0} x_{n} + b_{1} x_{n-1} + \cdots + b_{N} x_{n-N} = \sum_{i=0}^{N} b_{i} x_{n-i}
\end{equation}
\vspace{1mm}

\noindent where $x$ and $y$ are the input and output signals, respectively, and $b_i$ is the value of the impulse response at time instant $i$. The inputs $x_{n-i}$ are sometimes referred to as ``taps'', since they \textit{tap into} the input signal at various time instants. 

The FIR filter can be implemented in C as:

\begin{code}
void fir(int N, int L, double *b, double *x, double *y) {
 int j, k;
 double tap[256];
 for(j=0; j<N; j++) tap[j] = 0.0;
 for(j=0; j<L; j++) {
  for(k=N; k>1; k--) tap[k-1] = tap[k-2];
  tap[0] = x[j];
  y[j] = 0.0;
  for(k=0; k<N; k++) y[j] += b[k] * tap[k]; (#\label{line:dot}#)
 }
}
\end{code}

\noindent where $N$ is the filter rank, $L$ is the size of the input, and $b$, $x$, and $y$ are pointers to the filter's coefficients, input, and output, respectively.

At first glance, the C code seems to be a good representation of the FIR filter, but there is a few problems with its implementation. For example, the for-loop on line~\ref{line:dot} calculates a dot-product of the arrays $b$ and $tap$ inline. While it is possible to extract the computation like so:

% Implementing a dot-product in this manner will tie it to our FIR filter...

\begin{code}
double dot(int N, double *xs, double *ys) {
  double sum = 0;
  for (int i=0; i<N; i++) sum += xs[i] * ys[i];
  return sum;
}
\end{code}

\noindent \codei{dot} is still specialized to values of type $double$, it assumes $b$ and $tap$ both have at least $N$ elements, and it is not compositional in the sense that it cannot be merged with the producers of \codei{xs} or \codei{ys} without looking at their implementation first.

A dot product can be implemented in our embedded co-design language, as shown in section~\ref{intro}, using a similar imperative, but not idiomatic, style:

\begin{code}
dotSeq :: Arr Float -> Arr Float -> Program (Exp Float)
dotSeq x y = do
  sum <- initRef 0
  for 0 (min (length x) (length y) $ \ix -> do
    a <- getArr x ix
    b <- getArr y ix
    modifyRef sum $ \s -> s + a * b
  getRef sum
\end{code}

\noindent A program in the co-design languages is a monad: a kind of composable computation description. Together with Haskell's \codei{do} notation, a monad allows for instructions to be sequenced in much the same way as in C.

The above dot product is also not without fault. For instance, as with the C version, it is limited to values of type \codei{Float}; a dot product can be performed over any numerical value, as long as it supports addition and multiplication. Fortunately, functions over restricted types can be generalized to accept a range of values through Haskell's type classes. For numerical values, the type class \codei{Num} is used:

% , and we change the function's type to accept any value that satisfies \codei{Num} while leaving its body unchanged:

\begin{code}
dotSeq :: Num a => Arr a -> Arr a -> Program (Exp a)
\end{code}

\noindent \codei{dotSeq} is now polymorphic in the kind of values it accepts, but also limited to numerical. Even with its new type, the function is still quite fragile: the for-loop's length and the array indexing are both handled manually. That is, a single typo in any of these two would break the function but not its type, creating an error that would first emerge at run-time.

For purely array based computations like the dot product, the idiomatic approach would instead be to use our vector language:

% Where users can build larger functions from its library of smaller, pre-verified functions. A dot product can be defined as follows, using functions from this vector library:

\begin{code}
dotVec :: Num a => Vec a -> Vec a -> Exp a
dotVec xs ys = sum (zipWith (*) xs ys)
\end{code}

\noindent Here, the dot product is calculated by first joining the two vectors \codei{xs} and \codei{ys} by element-wise multiplication with \codei{zipWith}, and then reducing the resulting vector with \codei{sum}.

% Of these two functions, \codei{zipWith} is an example of a higher-order function, like the \codei{map} function from section~\ref{functional}; the manner in which it joins the two vectors is determined by the function given as its first argument.

A dot product based on vectors is not only closer to its mathematical specification than its sequential version, but also sturdier in the sense that it is harder for users to make an error: indices and lengths are now hidden by pre-verified vector functions. Furthermore, Haskell's lazy evaluation ensures that \codei{dotVec} can be merged freely with the producers of \codei{xs} and \codei{ys}: say, for example, that \codei{xs} is a constant vector of ones, \codei{zipWith} is then able to replace all indexing into \codei{xs} with a constant one.

As an example, we connect the dot product to a small program and compile it to C:

\begin{code}
int main() {
  uint16_t _a0[] = {1, 2, 3, 4}, *a0 = _a0;
  uint16_t _a1[] = {4, 3, 2, 1}, *a1 = _a1;
  uint16_t state2 = 0;
  uint32_t v3;
  for (v3 = 0; v3 < 4; v3++)
    state2 = a0[v3] * a1[v3] + state2;
  fprintf(stdout, "result: %d\n", state2);
  return 0;
}
\end{code}

\noindent The software program connects our dot product to two arrays of four unsigned integers and prints its result to standard output. Imports are omitted for brevity.

For a full implementation of the FIR filter, the vector language is a bit outside its comfort zone: vectors excel at describing array transformations, whereas the filter is described by a recurrence equation where output depends on previous input values. Nevertheless, the vector library does provide a few such recurrence functions, and we use one of them to implement the full filter:

\begin{code}
firVec :: Num a => Vec a -> Vec a -> Program (Arr a)
firVec cs v = recurrenceI (replicate (length cs) 0) v $ \i -> dotVec cs i
\end{code}

\noindent \codei{recurrenceI} takes an initial buffer, an input vector to iterate over, and a step function that produces one output at a time from the previous inputs and buffer. Note that \codei{reccurenceI} is not a combinator like the earlier vector functions, as it consumes the two input vectors to produce its output array. Different recurrence schemes would make use of other functions. 

% Recurrence functions can handle the regular feedback of the FIR filter quite well, but struggles for irregular access to earlier inputs. Nevertheless,

Compiling the \codei{firVec} to C yields the following code:

\begin{code}
int main() {    
  r5 = 0;
  for (v6 = 0; v6 <= 3; v6++) {
    a3[r5] = a1[v6];
    r5 = (r5 + 1) % 4;
    state7 = 0;
    for (v8 = 0; v8 < 4; v8++)
      state7 = a0[v8] * a3[(4 + r5 - v8 - 1) % 4] + state7;
    a2[v6] = state7;
  }
}
\end{code}

\noindent Imports, definitions, and code unrelated to the filter have been omitted for the sake of brevity. Note that \codei{a1} contains the input array and \codei{a0} the coefficients, both have a length of four, and array \codei{a3} is the queue used to store inputs. However, as all inputs present at once \codei{firVec}, it is possible to do without the queue:

% Seeing as how all the filters inputs is present at once in the input vector, we could double down on the vector based approach and do without the queue:

\begin{code}
firQ :: Num a => Vec a -> Vec a -> Vec a
firQ coeff = map (dotVec coeff . reverse) . tail . inits
\end{code}

\codei{firQ} trades the queue used by \codei{firVec}, and thus any modulo operations used during indexing, for a few stub-vectors that index directly into the input vector: \codei{inits} creates a vector from each initial segment of the input and \codei{tail} drops the first of these vectors. Compilation yields the following C code:

\begin{code}
int main() {
  for (v3 = 0; v3 <= 3; v3++) {
    if (4 <= v3 + 1) { b5 = 4; } else { b5 = v3 + 1; }
    if (4 <= b5) { b4 = 4; }
    else {
      if (4 <= v3 + 1) { b6 = 4; } else { b6 = v3 + 1; }
      b4 = b6;
    }
    state7 = 0;
    for (v8 = 0; v8 < b4; v8++) {
      if (4 <= v3 + 1) { b9 = 4; } else { b9 = v3 + 1; }
      state7 = a0[v8] * a1[b9 - v8 - 1] + state7;
    }
    a2[v3] = state7;
  }
}
\end{code}

While a FIR filter can be described using vectors, an idiomatic approach for such functions is to instead use our signal processing language. The language is built upon the co-design language and introduces the concept of signals: possibly infinite sequences of values. Like the vector language, idiomatic signal functions are constructed compositionally using smaller functions, but signals also provide a function for introducing unit delays.

As an example, we create three signal functions that represent the main components of a FIR filter:

\begin{code}
sums :: Num a => [Sig a] -> Sig a
sums as = foldr1 (+) as

muls :: Num a => [Exp a] -> [Sig a] -> [Sig a]
muls as bs = zipWith (*) (map constant as) bs

dels :: Exp a -> Sig a -> [Sig a]
dels e as = iterate (delay e) as
\end{code}

\noindent That is, a summation and a multiplication with coefficients, which together form a dot product, and a number of successive delays to form the taps.

We should note that \codei{foldr1}, \codei{zipWith}, \codei{map} and \codei{iterate} are the standard Haskell functions for lists, as opposed to signal functions. Addition and multiplication are lifted to operate element-wise over signals. \codei{constant} and \codei{delay} are proper signal functions and introduce a constant signal and a unit delay, respectively.

A full FIR filter can now be expressed with signals as:

\begin{code}
firSig :: Num a => [Exp a] -> Sig a -> Sig a
firSig coeffs = sums . muls coeffs . dels 0
\end{code}

\noindent which also is quite close to the filter's mathematical specification. Especially so from a hardware designers perspective: the input signal is delayed to form the filter's taps, where each ``tap'' is multiplied with a coefficient and summed. Hardware designers tend to be comfortable with this kind of box design, where each box corresponds to a function.

Compiling \codei{firSig} to VHDL produces the following hardware design:

% TODO: Might be an error in the code, as both processes trigger on the clock.
\begin{code}
ENTITY comp0 IS
  PORT (in0 : IN unsigned (7 DOWNTO 0);
        out1 : OUT unsigned (7 DOWNTO 0);
        clk : IN std_logic;
        rst : IN std_logic) ;
END ENTITY comp0 ;
ARCHITECTURE behav OF comp0 IS
  SIGNAL state2 : unsigned (7 DOWNTO 0) ;
  SIGNAL state2_d : unsigned (7 DOWNTO 0) := "00000000" ;
BEGIN
  l8 :
    PROCESS (in0) IS
      VARIABLE v3, v4, v5, v6, v7 : unsigned (7 DOWNTO 0) ; 
    BEGIN
      v3 := "00000001" ;
      v4 := "00000010" ;
      v5 := resize (v3 * ins0, 8) ;
      v6 := resize (v4 * state2_d, 8) ;
      v7 := resize (v5 + v6, 8) ;
      state2 <= in0 ;
      out1 <= v7 ;
    END PROCESS l8 ;
  l9 :
    PROCESS (clk) IS
    BEGIN
      IF rising_edge (clk) THEN
        state2_d <= state2 ;
      END IF ;
    END PROCESS l9 ;
END ARCHITECTURE behav ;
\end{code}

\section{Summary}

Section~\ref{intro} gave a general introduction to embedded programming and its challenges. Specifically, the problem of extracting performance from an embedded system. Heterogeneous system is then introduced as a possible response with potential, where section~\ref{background} mentioned modern FPGAs in particular as a heterogeneous system of interest.

Heterogeneous systems are not without their own challenges, as the presence of multiple processors raises all of the issues involved with parallel, homogeneous systems. Also, the level of heterogeneity in a system can introduce additional challenges with different system capabilities and development between processors: components may support different instructions, leading to incompatibilities between the code they can execute even if they're both programmed in the same language.

Functional languages were then introduced in section~\ref{functional} as a solution to the various modularity issues with using lower-level languages like C or VHDL for heterogeneous systems. Particularly the ``glue code'' of functional languages, that is their higher-order functions, type-system, and lazy evaluation, was shown to be useful for developing reusable components. Section~\ref{domain} showed how to embedded a language in Haskell, a functional programming language, and how these embedded languages benefit from the aforementioned benefits as well.

Section~\ref{embedded} went on to introduce our current attempt at bringing the benefits of functional programming languages to the domain of embedded heterogeneous systems with our hardware software co-design, vector and signal languages. The aim is to have the co-design language serve as a convenient description of imperative program, both software and hardware, with support for compilation to C and VHDL. On the other hand, our vector and signal language will serve as convenient front-ends for the co-design language, extending it with support for array and synchronous data-flow programming, respectively.

The remainder of this thesis goes over the languages in detail to highlight the various ideas they are built on. In particular, the following contributions are made:

\begin{itemize}
\item We present a language for hardware software co-design that is embedded in Haskell and designed with FPGA programming in mind. As such, the co-design language generates both C and VHDL code to describe its software and hardware components, including the necessary glue code for connections between components. Intrinsics of a component do however vary between components, even those described by the same language. Both our software and hardware languages are therefore extensible so that they may be of use in other systems as well.

\item We present two extensions to the hardware software co-design language. One extension supplements an array type with vector computations that can be defined in a compositional manner that supports fusion. The other extension supplements an expression type with synchronous data-flow and provides its own interpreter for turning such expressions into signal processing networks. Neither extension is dependent on the co-design language and can be fitted for use in other embedded languages.

\item We present the type-based techniques for implementing an embedded language like the co-design language. The technique is based on a monadic representation of imperative programs that is loosely coupled to its expression and predicate types, allowing each part to be developed separately. As an additional benefit, handling the sequencing of programs separately enables their interpretation to be handled separately as well. Programs and their interpretation are both designed with extensibility in mind.

\item We present a code generation scheme for programs by a series of translations between progressively smaller languages, where each step is typed in order to safeguard against common errors found in untyped translations. This scheme allows for languages to provide users with feature-rich expressions, while still having fine control over the generated source code.
\end{itemize}

\end{document}

%%  LocalWords:  DSLs EDSLs
