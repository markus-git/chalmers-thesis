%!TEX root = ../main.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       oe    
%     .@88    
% ==*88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
% '**%%%%%%** 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

Over the last few years, the amount of traffic going back and forth between devices in the global communications infrastructure has been increasing at a rapid pace. As the steady growth of mobile users and internet of things devices continues~\cite{ericsson2016}, the amount of mobile traffic is projected to become even greater. In fact, global internet traffic is estimated to grow at an average rate of twenty two percent annually, reaching approximately two hundred and fifty million terabytes per month by the end of 2021~\cite{cisco2016}. For the communication infrastructure, the consequence of such a rapid growth rate has been a sharp increase in the demand for computational power on systems that already run under tight latency constraints and with limited memory~\cite{persson2014}, which means computations have to be efficient.

As important as it is to increase the computational power of embedded systems used in communication, limiting their power consumption presents an equally important issue in their architectural design~\cite{mudge2001}. The trend of trading power for performance cannot continue indefinitely: typical house hold processors of today have a power density of 70W and upwards, which is seven times that of a typical hot plate. Containing the growth in power requires architectural improvements, with specialized computing for specialized tasks. Heterogeneous computing represents an interesting development towards the goal of energy efficient computing, and refers to systems that use more than one kind of processing units. These heterogeneous systems gain their performance and energy efficiency not just by combining several processors, but rather by incorporating different kinds of co-processors that provide specialized processing capabilities to handle a particular task.

Heterogeneous computing present new challenges in software design that are not found in the development for typical homogeneous systems~\cite{kunzman2011}. The multiple processing units present in a heterogeneous system raises all of the issues associated with homogeneous parallel systems, while the heterogeneity in the system gives rise to new issues due to  dissimilarity in system development and capability. The efficiency and computational power of heterogeneous systems thus comes at a cost of increased programming burden in terms of code complexity and portability, as hardware specific code is interleaved with application code to handle any communication between co-processors. Furthermore, the structure of application code typically vary between co-processors and any code written for one therefore requires modification when given a new target. In fact, despite all the advantages heterogeneous systems offer, their use so far has been mostly restricted to hardware programmers.

A substantial amount of research has gone into addressing the challenges of programming for embedded heterogeneous systems, opening them up for programmers without a background in hardware or embedded system design. However, hardware description languages, such as VHDL and Verilog, are still the most commonly used tools, together with C for specific co-processors. These hardware description languages have revolutionized hardware design but suffer from a lack of expressiveness and standardization -- there is a mismatch between description and synthesized hardware. Designers have therefore looked for alternative solutions, where one of the more well-known approaches is synthesis of high-level languages like C~\cite{graphics2008, ghenassia2005}. Compiling high-level languages to a hardware description has great appeal, but finding a translation between the two has however proven to be difficult; sequential programs are often a bad fit for the parallelism inherent to most hardware architectures.

Another group of languages whom I believe have shown success in describing hardware designs are functional languages~\cite{sheeran2005}. Higher-order functional languages in particular, where hardware descriptions are first-class objects, offer a particularly useful abstraction mechanism~\cite{baaij2010, bjesse1998, gill2010}. Another beneficial attribute of these languages is their purity, which enables reasoning about function (de-)composition. \todo{More?}

% Functional languages are however rarely considered for embedded system development, as its difficult to give performance guarantees and resource bounds for its programs.

This thesis presents the first steps towards a functional programming language in which the entire design process of heterogeneous systems can be expressed. I do however intend to go further than solely describing hardware; some components are better described using sequential algorithms directly, rather than having one generated from a hardware description. I also intend to support the necessary design exploration to decide where the boundary between hardware and software should be drawn. \todo{More?}

This thesis consists of two parts. Part I is a general introduction to the field and puts the appended papers into context. Part II contains the appended papers.

% Limit to FPGAs.

\section{Background}

High demands for efficiency under resource constraints have greatly influenced the development of embedded systems used in communications infrastructure, both in terms of programming practice and architecture. Today, we see embedded systems consisting of everything between general purpose processors (GPPs) and application specific integrated circuits (ASICs). GPPs are highly programmable systems but often inefficient in terms of power consumption and performance. In contrast, ASICs implement a fixed function and can therefore provide good power and performance characteristics, but any changes to its functionality requires a new circuit to be designed. 

Processors and ASICs represent two extremes of available architectures, but fortunately there exists several other architectures in between. Field programmable gate arrays (FPGAs) are one such kind, and provides the best of both worlds: they are close to hardware and can be reprogrammed~\cite{bacon2013}. FPGAs typically consists of a large array of configurable logic blocks, connected by programmable interconnects. However, a modern FPGA also contain various discrete components and co-processors, which together with their good performance per Watt ratio, have seen them increasingly used in high-performance, computationally intensive systems~\cite{mcmillan2014}.

\subsection{Difficulties of heterogeneity}

While modern FPGAs show great promise as a prototypical system for heterogeneous computing, their adoption has been slowed by the fact that they're difficult to program. The logic blocks are usually programmed in a hardware description language, while the embedded processors and accelerators are typically programmed in a low level dialect of C or assembler.  This choice of low level languages is primarily driven by the desire to access the full potential of a processor or its memory system. However, low level languages also forces its developers to focus on low level implementation details rather than the high level specification of the algorithm they are implementing.

Furthermore, in the design of a heterogenous systems there's also concerns related to any dissimilarities between the embedded components that needs to be address. Components may support different instructions, leading to incompatibilities between the code they can execute even if they're both programmed in the same language. Even the implementation of language features, such as functions, may differ between components. In addition to instructions, components may interpret memory in different ways and have access to various cache structures. These differences imply that a heterogeneous system may have components that are similar in architecture, but have low-level differences that to variations in performance and power consumption.

One of the benefits of low level code is that it gives programmers fine control over the aforementioned system capabilities and differences thereof. However, this control comes at a cost, as the programmers must exercise this right during the entire design process. So the problem of implementing an algorithm has become a problem of implementing an algorithm for a specific component. In heterogeneous system design, the developer must specify both hardware and software parts, and how they communicate; ideally she would like to experiment with various choices of what to put in hardware and what in software. Low-level languages provide little support for such design exploration, and rewriting code intended for component to another is a major undertaking.

\section{The Co-Feldspar language}

\lipsum[1]

\subsection{Functional programming}

\lipsum[2]

\subsection{Domain Specific Languages}

\lipsum[3]

\subsection{Domain Specific Embedded Languages}

\lipsum[4]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .--~*teu.
%  dF     988Nx
% d888b   `8888>
% ?8888>  98888F
%  "**"  x88888~
%       d8888*`
%     z8**"`   :
%   :?.....  ..F
%  <""888888888~
%  8:  "888888*
%  ""    "**"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Co-Design}

\lipsum[5]

\section{Section about Expression}

\lipsum[1]

\begin{code}
square :: SExp Int32 -> SExp Int32
square a = a * a
\end{code}

\lipsum[1]

\begin{stub}
square :: HExp Int32 -> HExp Int32
square a = a * a
\end{stub}

\lipsum[1]

\begin{stub}
square :: (Multiplicative exp, Type' exp a, Num a) => exp a -> exp a
square a = a * a
\end{stub}

\lipsum[1]

\begin{code}
type Point a = (a, a)

pair :: (Expr exp, Type' exp a, Num a) => Point (exp a) -> Point (exp a) -> exp a
pair (a, b) (u, v) = (a + b) * (u + v)
\end{code}

\lipsum[1]

\begin{code}
dotProd :: (Expr exp, Type' exp a, Num a) => Pull exp a -> Pull exp a -> exp a
dotProd xs ys = forLoop n 0 $ \i s -> s + xs!i * ys!i
  where
    n = min (length xs) (length ys)
\end{code}

\lipsum[1]

\begin{stub}
forLoop :: Syntax exp st => exp Length -> st -> (exp Index -> st -> st) -> st
\end{stub}

\lipsum[1]

\begin{code}
zipWith :: Expr exp => (a -> b -> c) -> Pull exp a -> Pull exp b -> Pull exp c
zipWith f xs ys = fmap (uncurry f) (zip xs ys)

sum :: (Expr exp, Type' exp a, Num a) => Pull exp a -> exp a
sum = fold (+) 0
\end{code}

\lipsum[1]

\begin{code}
scProd :: (Expr exp, Type' exp a, Num a) => Pull exp a -> Pull exp a -> exp a
scProd a b = sum (zipWith (*) a b)
\end{code}

\section{Section about Programs}

\lipsum[1]

\begin{code}
hello :: Software ()
hello = printf "Hello world!\n"
\end{code}

\lipsum[2]

\begin{code}
hello :: Software ()
hello = printf "Hello world!\n"
\end{code}

\lipsum[3]

\begin{code}
reverse :: SArr Int32 -> Software ()
reverse arr =
  do for 0 (len `div` 2) $ \ix ->
       do aix <- getArr arr ix
          ajx <- getArr arr (len - ix)
          setArr arr ix         ajx
          setArr arr (len - ix) aix
  where
    len = length arr
\end{code}

\lipsum[4]

\begin{stub}
reverse :: HArr Int32 -> Hardware ()
\end{stub}

\lipsum[5]

\begin{stub}
reverse :: (Arrays m, Expr (Exp m), Type' (Exp m) Int32) =>
  Arr m (Exp m Int32) -> m ()
\end{stub}

\lipsum[6]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .x~~"*Weu.
%  d8Nu.  9888c
%  88888  98888
%  "***"  9888%
%       ..@8*"
%    ````"8Weu
%   ..    ?8888L
% :@88N   '8888N
% *8888~  '8888F
% '*8"`   9888%
%   `~===*%"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Concluding Remarks}
\label{ch:conc}

\lipsum
