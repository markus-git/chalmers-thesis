%!TEX root = ../main.tex

% TODO:
%  - support C style lstset as well as Haskell.
%  - test new examples.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       oe    
%     .@88    
% ==*88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
% '**%%%%%%** 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

Over the last few years, the amount of traffic going back and forth between devices in the global communications infrastructure has been increasing at a rapid pace. As the steady growth of mobile users and internet of things devices continues~\cite{ericsson2016}, the amount of mobile traffic is projected to become even greater. In fact, global internet traffic is estimated to grow at an average rate of twenty two percent annually, reaching approximately two hundred and fifty million terabytes per month by the end of 2021~\cite{cisco2016}. For the communication infrastructure, the consequence of such a rapid growth rate has been a sharp increase in the demand for computational power on systems that already run under tight latency constraints and with limited memory~\cite{persson2014}, which means computations have to be efficient.

As important as it is to increase the computational power of embedded systems used in communication, limiting their power consumption presents an equally important issue in their architectural design~\cite{mudge2001}. The trend of trading power for performance cannot continue indefinitely: typical house hold processors of today have a power density of 70W and upwards, which is seven times that of a typical hot plate. Containing the growth in power requires architectural improvements, with specialized computing for specialized tasks. Heterogeneous computing represents an interesting development towards the goal of energy efficient computing, and refers to systems that use more than one kind of processing units. These heterogeneous systems gain their performance and energy efficiency not just by combining several processors, but rather by incorporating different kinds of co-processors that provide specialized processing capabilities to handle a particular task.

Heterogeneous computing present new challenges in software design that are not found in the development for typical homogeneous systems~\cite{kunzman2011}. The multiple processing units present in a heterogeneous system raises all of the issues associated with homogeneous parallel systems, while the heterogeneity in the system gives rise to new issues due to  dissimilarity in system development and capability. The efficiency and computational power of heterogeneous systems thus comes at a cost of increased programming burden in terms of code complexity and portability, as hardware specific code is interleaved with application code to handle any communication between co-processors. Furthermore, the structure of application code typically vary between co-processors and any code written for one therefore requires modification when given a new target. In fact, despite all the advantages heterogeneous systems offer, their use so far has been mostly restricted to hardware programmers.

A substantial amount of research has gone into addressing the challenges of programming for embedded heterogeneous systems, opening them up for programmers without a background in hardware or embedded system design. However, hardware description languages, such as VHDL and Verilog, are still the most commonly used tools, together with C for specific co-processors. These hardware description languages have revolutionized hardware design but suffer from a lack of expressiveness and standardization -- there is a mismatch between description and synthesized hardware. Designers have therefore looked for alternative solutions, where one of the more well-known approaches is synthesis of high-level languages like C~\cite{graphics2008, ghenassia2005}. Compiling high-level languages to a hardware description has great appeal, but finding a translation between the two has however proven to be difficult; sequential programs are often a bad fit for the parallelism inherent to most hardware architectures.

Another group of languages whom I believe have shown success in describing hardware designs are functional languages~\cite{sheeran2005}. Higher-order functional languages in particular, where hardware descriptions are first-class objects, offer a particularly useful abstraction mechanism~\cite{baaij2010, bjesse1998, gill2010}. Another beneficial attribute of these languages is their purity, which enables reasoning about function (de-)composition. \todo{More?}

% Functional languages are however rarely considered for embedded system development, as its difficult to give performance guarantees and resource bounds for its programs.

This thesis presents the first steps towards a functional programming language in which the entire design process of heterogeneous systems can be expressed. I do however intend to go further than solely describing hardware; some components are better described using sequential algorithms directly, rather than having one generated from a hardware description. I also intend to support the necessary design exploration to decide where the boundary between hardware and software should be drawn. \todo{More?}

This thesis consists of two parts. Part I is a general introduction to the field and puts the appended papers into context. Part II contains the appended papers.

% Limit to FPGAs.

\section{Background}
\label{background}

High demands for efficiency under resource constraints have greatly influenced the development of embedded systems used in communications infrastructure, both in terms of programming practice and architecture. Today, we see embedded systems consisting of everything between general purpose processors (GPPs) and application specific integrated circuits (ASICs). GPPs are highly programmable systems but often inefficient in terms of power consumption and performance. In contrast, ASICs implement a fixed function and can therefore provide good power and performance characteristics, but any changes to its functionality requires a new circuit to be designed. 

Processors and ASICs represent two extremes of available architectures, but fortunately there exists several other architectures in between. Field programmable gate arrays (FPGAs) are one such kind, and provides the best of both worlds: they are close to hardware and can be reprogrammed~\cite{bacon2013}. FPGAs typically consists of a large array of configurable logic blocks, connected by programmable interconnects. However, a modern FPGA also contain various discrete components and co-processors, which together with their good performance per Watt ratio, have seen them increasingly used in high-performance, computationally intensive systems~\cite{mcmillan2014}.

%\subsection{Difficulties of heterogeneity}

While modern FPGAs show great promise as a prototypical system for heterogeneous computing, their adoption has been slowed by the fact that they're difficult to program. The logic blocks are usually programmed in a hardware description language, while the embedded processors and accelerators are typically programmed in a low level dialect of C or assembler.  This choice of low level languages is primarily driven by the desire to access the full potential of a processor or its memory system. However, low level languages also forces its developers to focus on low level implementation details rather than the high level specification of the algorithm they are implementing.

Furthermore, in the design of a heterogenous systems there's also concerns related to any dissimilarities between the embedded components that needs to be address. Components may support different instructions, leading to incompatibilities between the code they can execute even if they're both programmed in the same language. Even the implementation of language features, such as functions, may differ between components. In addition to instructions, components may interpret memory in different ways and have access to various cache structures. These differences imply that a heterogeneous system may have components that are similar in architecture, but have low-level differences that lead to variations in performance and power consumption.

One of the benefits of low level code is that it gives programmers fine control over the aforementioned system capabilities and differences thereof. However, this control comes at a cost, as the programmers must exercise this right during the entire design process. So the problem of implementing an algorithm has become a problem of implementing an algorithm for a specific component. In heterogeneous system design, the developer must specify both hardware and software parts, and how they communicate; ideally she would like to experiment with various choices of what to put in hardware and what in software. Low-level languages provide little support for such design exploration, and rewriting code intended for component to another is a major undertaking.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .--~*teu.
%  dF     988Nx
% d888b   `8888>
% ?8888>  98888F
%  "**"  x88888~
%       d8888*`
%     z8**"`   :
%   :?.....  ..F
%  <""888888888~
%  8:  "888888*
%  ""    "**"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Co-Design Language}

Many of the issues in section~\ref{background} that one faces in heterogenous computing with low level languages like C and HDLs are related to a lack of abstractions. Some of these languages modularity problems come as a direct result of the fine grained control they provide, as it will inevitably tie programs to the architecture of its system. Some issues come as a indirect consequence of the lack of abstractions. For instance, we would like to treat functionality like parallelism as a seperate design decision. In imperative languages like C, this is however quite difficult to do. HDLs contrast this by assuming programs to be parallel by default.

Issues with differences in language design between imperative languages and HDLs can also be ascribed to a lack of abstractions. Ideally, issues of functionality, architecture or parallelism would be treated separately. As being able to do so would allow us to create reusable libraries that provide solutions to these issues, and then combine them in a modular fashion to solve problems.

\subsection{Functional Programming}
\label{functional}

Functional programming, as the name implies, is based around the application of functions to its arguments. In this programming style, programs are written as functions that accept input and delivers its result. These functions are themselves defined in terms of smaller functions, which in turn are defined using smaller functions still and, in the end, a function consists of language primitives such as built-in functions, variables or constants. An important distinction between functions in a functional programming language and, say an imperative language like C, is that functions always return the same value when given same arguments. More generally, we say that functional programs have no side effects and their definitions and evaluation are thus separated; functions can safely be evaluated in parallel as long at their data dependencies are satisfied.

A function that accepts other functions as arguments is often referred to as a higher-order function, or a combinator, and provide a useful piece of glue code that lets programmers build complex functions from smaller ones. In Haskell, a functional programming language, a number of such higher-order functions that implement common operations are provided by its standard libraries. One such function is \codei{map} and it can be defined as follows:

\begin{code}
map :: (a -> b) -> [a] -> [b]
map f []     = []
map f (x:xs) = f x : map f xs
\end{code}

The first line specifies the type of \codei{map}, because in Haskell, every function is assigned a static type in an effort to attain safer programs. If you by mistake write a program that tries to multiply some integer by a boolean type, it won't even compile and instead warns you about the type error. As for the types themselves, they are a kind of label that every expression has and states what category of operations that the expression belongs in.

A function's type comes after the \codei{::} sign, and in the case of \codei{map}, tells us that its first argument is a function \codei{f :: a -> b} which, given an argument of type \codei{a}, produces a result of type \codei{b}. In addition to the function, \codei{map} also takes a list \codei{xs :: [a]} of element with type \codei{a}, and returns another list of elements with type \codei{b}. As functions cannot have any side effects, we can already make a guess at what this function does and claim that it applies \codei{f} to every element of \codei{xs}.

The second and third line of \codei{map} validates our earlier guess and lists the full definition of the function. Firstly, it says that given an empty list, shown as \codei{[]}, the result is another empty list as there's simply nothing to apply \codei{f} to. Secondly, in the case where \codei{map} is given a non-empty list \codei{x:xs} where \codei{x} is the list head and \codei{xs} its tail, it applies \codei{f} to \codei{x} and concatenates its result with a recursive call to itself on \codei{xs}.

The other piece of glue code that functional programming languages provides is often referred to as function composition, and enables programs to be glued together. Say that \codei{f} and \codei{g} are two programs, then \codei{g} composed with \codei{f} is written \codei{(g . f)} and is a program that, when applied to its input \cdoei{x}, computes \codei{g (f x)}. In Haskell, we can define function composition as:

\begin{code}
(.) :: (b -> c) -> (a -> b) -> a -> c
(.) g f x = g (f x)
\end{code}

\noindent where parenthesis around the dot implies that function composition is an infix function. While the size of the intermediate result of \codei{f} could spoil the usefulness of composition, functional programming solves this by only evaluating \codei{f} as much as is needed by \codei{g}. This property is referred to as lazy evaluation.

\subsection{Domain Specific Languages}

A domain specific language (DSL) is a special-purpose language, tailored to a certain problem and captures the concepts and operations in its domain. For instance, a hardware designer might write in VHDL, while a web-designer might write an interactive web-page using JavaScript. Both use a language that is customized to their particular task at hand, and both build programs in a form that is familiar to programmers; VHDL and JavaScript are both examples of a DSL.

DSLs comes in two fundamentally different forms: external and internal. An external DSL is a first-class language, with its own compiler or interpreter, and often comes with its own ecosystem. The previous VHDL and JavaScript examples both fall into this domain of DSLs. Internal DSLs are embedded in a host language, and are often referred to as embedded domain specific languages (EDSLs). Embedded languages can have the look and feel of a stand-alone language, but reuse many parts of the host-language's ecosystem and semantics to lower the cost to develop and maintain them.

Haskell, with its static type system, flexible overloading and lazy semantics, has seen itself as host for a range of EDSLs. For instance, popular libraries for parsing, pretty printing, hardware design and testing have all been embedded in Haskell. These embedded languages come in two different flavors, where the most common one is called shallow embedding and is represented by functions that implements the semantics of the embedded language. The advantage of this embedding is that calculating the value of a computation is very fast, as we just evaluate it to get its value.

Shallowly embedded languages do however perform quite poorly as a technique if we wish to compile the language since its functions return values. To compile an embedded language it is better to use an intermediate representation for its abstract syntax tree. This technique is known as a deep embedding, and lets us inspect a whole computation rather than its result. While the representation of embedded language's syntax tree can become quite unwieldy as the supported syntax grows, one is necessary if we would like to transform the representation, or if we have another back-end than compilation.

\todo{In our co-design language we make use of a combination of deep and shallow embeddings, having implemented the core syntax as a deep embedding with user facing libraries as shallow embeddings. This mixture means that the core language is easy to interpret while the shallow embeddings provides a nice user interface.}

\subsection{Embedded Programming in Haskell}
\label{haskell}

Programming in a functional language like Haskell is different from the imperative programming of C or the hardware descriptions written in VHDL. Ideally, functional languages allows users to write their program as a mathematical function, that is, a function from its inputs to its output. Which is in contrast to the imperative style of programming, where users express their program as a series of sequential steps to be executed. Furthermore, functional programming is typically done compositionally, and its languages provide a rich of operators to support the composition of new functions from smaller ones. As a result, fuctional languages tend to transform data in bulk operations rather than through iterative loops.

As an example of the above differences, we'll consider a finite impulse response (FIR) filter, one of the two primary types of digital filters used in digital signal processing applications~\cite{oppenheim1989}. A FIR filter is a filter whose impulse response has a finite duration, since it eventually settles to zero. This is in contrast to infinite impulse response filters, the second filter of the two primary types of digital filters, which may have internal feedback loops and thus an infinite impulse response. The mathematical definition of a FIR filter of rank $N$ is as follows:

\vspace{-2mm}
\begin{equation}
y_{n} = b_{0} x_{n} + b_{1} x_{n-1} + \cdots + b_{N} x_{n-N} = \sum_{i=0}^{N} b_{i} x_{n-i}
\end{equation}
\vspace{1mm}

\noindent where $x$ and $y$ are the input and output signals and $b_i$ is the value of the impulse response at time instant $i$. The inputs $x_{n-i}$ are sometimes referred to as ``taps'' as they tap into the input signal at various time instants. 

In an imperative language like C, we implement the FIR filter as:

\begin{code}
void fir(int N, int L, double *b, double *x, double *y)
{
 int j, k;
 double tap[256];
 for(j=0; j<N; j++) tap[j] = 0.0;
 for(j=0; j<L; j++)
 {
  for(k=N; k>1; k--) tap[k-1] = tap[k-2];
  tap[0] = x[j];
  y[j] = 0.0;
  for(k=0; k<N; k++) y[j] += b[k] * tap[k];
 }
}
\end{code}

\noindent Looking at the C code, it seems to be a good representation of the FIR filter. There are however a few problems with the implementation. For instance, the second of the two inner for-loops that calculates the intermediate result $v$ does so by performing a dot-product of the arrays $b$ and $tap$. Having implemented a dot-product in this manner does mean that it is specialized to the FIR filter, and any other function that needs to perform their own dot-product would have to reimplement it. While it is possible to extract the computation like so:

\begin{code}
double dot(int N, double *xs, double *ys)
{
  double sum = 0;
  for (int i=0; i<N; i++) sum += xs[i] * ys[i];
  return sum;
}
\end{code}

\noindent the function is still specialized to values of type $double$, it assumes $b$ and $tap$ both have at least $N$ elements, and it is not readily compositional: the function cannot be merged with producers of \codei{xs} or \codei{ys} without looking at its implementation.

In Haskell, functions like the dot-product can be implemented in a compositional style using two operations from its standard libraries, namely \codei{zipWith} and \codei{sum}:

\begin{code}
dot :: Num a => [a] -> [a] -> a
dot xs ys = sum (zipWith (*) xs ys)
\end{code}

\noindent Here, the first of the two helper functions, \codei{zipWith}, is used to combine the two input lists \codei{xs} and \codei{ys} into one by element wise multiplication -- \codei{zipWith} is a general, list-based function and can used with other operators than simply multiplication. The resulting list is summed together with the help of \codei{sum}, which produces the final result of the dot-product. In contrast to the C function, the above dot-product is able to take any numerical value instead of just doubles, thanks to its \codei{Num} constraint on \codei{a}. Haskell's lazy evaluation also ensures that \codei{dot} can be merged freely with the producers of \codei{xs} and \codei{ys}.

Now, with the help of our dot-product, we can implement the FIR filter as well:

\begin{code}
fir :: Num a => [a] -> [a] -> [a]
fir b xs = snd (mapAccumR step (replicate (length b) 0) xs)
  where
    step :: [a] -> a -> ([a], a)
    step ts i = (x, dot b x)
      where x = shift i ts
  
    shift :: a -> [a] -> [a]
    shift x xs = x : init xs
\end{code}

\noindent Here I've also factored out two additional components of the filter, namely the \codei{shift} and \codei{step} operations: \codei{shift} manages the filter's taps as it pushes a new value onto them while at the same time dropping their oldest value; \codei{step} implements the logic of the second for-loop of the C variant, that is, it shifts the current input onto the taps and then calculates the output for that time instant. As for the main filter body itself, a special mapping function called \codei{mapAccumR} is used, which is similar to the earlier \codei{map} function show in section~\ref{functional} but also carries around an extra value with it throughout the applications of \codei{step} to \codei{xs}. This extra value is used to hold the taps, and is initially a list of zeroes created by Haskell's \codei{replicate} function, but each application of \codei{step} will push an new input onto them. The final result of the filter is then obtained using \codei{snd}, which throws away the taps and returns the accumulated outputs.

Embedded programming in Haskell is quite similar to the kind of functional programming that we have done so far. In fact, we can reimplement the earlier dot-product as an embedded function in our co-design library without having to change its definition very much at all:

\begin{code}
dot :: (Num a, Type a) => Vec (Exp a) -> Vec (Exp a) -> Exp a
dot xs ys = sum (zipWith (*) xs ys)
\end{code}

\noindent Its type has grown a bit and is perhaps the clearest indicator of how things have changed. Gone are the regular Haskell lists and values, and in their place we have the vector and expression types \codei{Vec} and \codei{Exp}, respectively. These types are part of the co-design library, and provides operations that mimic common operations in Haskell. For example, vectors can be thought of as lists with a finite size, while expressions can be thought of as a computation that produces a regular value. It is in fact possible to evaluate an expression to a regular Haskell value:

\begin{code}
evaluateExpr :: Type a => Exp a -> a
\end{code}

\noindent as long as its an expression over some known type \codei{a} that supports evaluation, as enforced by the \codei{Type} constraint.

Evaluating expressions into their matching Haskell value is however not the only supported operation, we could just as well compile the same expression to, say, its equivalent C code. These different kinds of interpretations of expressions is possible because they're an example of a deeply embedded type, and operations over them only build an interim data structure that reflects the computation behind the expression. For instance, an expression like \codei{(a * b)} could result in an abstract syntax tree of \codei{(Mult (Var ``a'') (Var ``b''))} in Haskell. It is this syntax tree that we can inspect, modify and interpret in different ways, all from within Haskell. % Compiling the dot-product function to C results in the following code being generated:

The similarities between the implementations of our embedded dot-product and the earlier one are mostly superficial, as the helper functions used in the embedded variant are not the standard Haskell functions we used before, but rather functions provided by our vector library that share the same names:

\begin{code}
data Vec a = Vec (Exp Length) (Exp Index -> a)

zipWith :: (Type a, Type b) => (a -> b -> c) -> Vec a -> Vec b -> Vec c
zipWith f a b = map (uncurry f) (zip a b)

sum :: (Num a, Type a) => Vec a -> a
sum = fold (+) 0
\end{code}

\noindent What's interesting about vectors is that they're defined in terms of a regular Haskell data type with two components: a length, given by the expression \codei{Exp Length}; and an indexing function \codei{(Exp Index -> a)} that, given an index of type \codei{Exp Index}, looks up the value \codei{a} of the vector. \codei{Index} and \codei{Length} are both integer types in disguise that have been renamed to make them more descriptive.

Vectors, when implemented in the above manner, are an example of a shallowly embedded data type. Its no coincident that the definition of vectors is similar to their \todo{semantics}, as the very idea of shallow embeddings is that the representation they use model the operations that can be performed on them. This characteristic of shallow embeddings makes it \todo{easy} to add new language constructs, like a \codei{mapAccum} for vectors, but also makes them difficult to interpret in any other way than the intended one. To get around this issue, vectors are based on the deeply embedded \codei{Exp} type, and can be interpreted as one big expression. Compiling the embedded dot-product to C yields the following code:

\begin{code}
-- todo: compile dot-product to C and past code here.
\end{code}

\todo{Text that has to be here or my latex stops working. By the way, the text above this might not be super relevant for this section.}

\section{Co-Design}
\label{codesign}

In section~\ref{haskell} I talked about generating C code from our embedded Haskell programs, but for the kind of heterogeneous computing that our library aims to describes only C is not sufficient. Typically, heterogeneous system see hardware code interleaved with application code, and even components described by the same language can support different intrinsic operations. So, starting with a single Haskell program, our co-design library is designed with three main tasks in mind: generate C code for the application parts, generate VHDL for the hardware parts, and generate a combination of C and VHDL to enable the transmission of data between software and hardware components. Furthermore, the software and hardware languages are extensible to support the addition of various intrinsic operations for different components.

C and VHDL are both imperative. To represent them, our co-design library builds on a deep embedding of monads. Monads can be thought of as composable computation descriptions, that is, they provide a means to connect smaller computations into a larger program. The general idea is that one can view an imperative program as a sequence of instructions to be executed on some machine, which looks similar to programs written in a stateful monad. In fact, a stateful program composed with monadic operations can be directly translated into statements in an imperative language. As an example, consider a software program for reversing an array:

\begin{code}
reverseS :: SArr Int32 -> Software ()
reverseS arr =
  for 0 (len `div` 2) $ \ix -> do
    aix <- getArr arr ix
    ajx <- getArr arr (len - ix)
    setArr arr ix ajx
    setArr arr (len - ix) aix
  where
    len = length arr
\end{code}

\noindent Its type tells us that it takes an array over 32-bit integers as input and produces a software program. As the array is reversed in place, the return type of the software program is simply empty, which in Haskell is show as \codei{()}. Also, the array type \codei{SArr} is prefixed with an \codei{S} to show that it is intended to be used with software statements. % an generic array type will be introduced a bit later on.

The implementation of our \codei{reverseS} function has the look and feel of a imperative program, sans a few syntactical differences. Firstly, the function performs a for-loop over the indices from zero and up to half of the arrays length. Then, for each such index \codei{ix}, it reads the array values at that index and the one from the opposite side. Lastly, these two values are put back into the array at the opposite values' index, resulting in a reversed array once the two indexed values meet in the middle. The two array operations, \codei{getArr} and \codei{setArr}, behave as you would expect them to: \codei{getArr} takes an array and an index, and returns the array's value at that index; \codei{setArr} takes an array, an index and a value, and stores the value in the array.

While the type of \codei{reverseS} is that of a software function, there's nothing software specific about it. For-loops and arrays are both part of most imperative languages, including C and VHDL. We could therefore have implemented the reverse function in hardware as well, and we do so by simple changing its type:

\begin{code}
reverseH :: HArr Int32 -> Hardware ()
reverseH arr = undefined -- same as before.
\end{code}

\noindent Other than its type, \codei{reverseH} has the same implementation as \codei{reverseS}.

Being able to implement both \codei{reverseS} and \codei{reverseH} using the same functions hints that us marking them as software and hardware specific programs was unnecessary. We would be better of keeping the reverse function generic, enabling it do be used in both languages. After all, the function only require its implementer to support for-loops and arrays to use it. The co-design library supports this kind of language agnostic functions by employing a hierarchy of type classes over operations like arrays or for-loops, with monads as the base. In the case of our reverse function, we would give it the following type:

\begin{code}
reverse :: (Monad m, Arrays m, Control m, Type m Int32) => Arr m Int32 -> m ()
reverse arr = undefined -- same as before.
\end{code}

The previous array types of \codei{SArr} and \codei{HArr} have been substituted for a more general type of \codei{Arr}, which is parameterized on the monad \codei{m}. The idea is that, a type of \codei{Arr} will turn into a \codei{SArr} or \codei{HArr} type when the monad is instantiated as \codei{Software} or \codei{Hardware}, respectively. The added constraints of \codei{Arrays} and \codei{Control} on \codei{m} ensures that whichever monad \codei{m} gets instantiated as will support the necessary array and control operations.

While \codei{Software} and \codei{Hardware} both support the previous stack operations, we can also point out groups of operations which are only supported by some interpretations but not by others -- like processes in hardware or IO in software. Monads still form the base of the hierarchy, but operations intended for either software or hardware branches also require that their monad is an extension of their respective \codei{Software} and \codei{Hardware} monads.

%\begin{code}
%class Monad m => Arrays m where
%  type Arr m :: * -> *
%  newArr  :: SyntaxM m a => Exp m Length -> m (Arr m a)
%  getArr  :: SyntaxM m a => Arr m a -> Exp m Index -> m a
%  setArr  :: SyntaxM m a => Arr m a -> Exp m Index -> a -> m ()
%\end{code}

\subsection{Expressions}

A language embedding based on monads gives users a representation of statements in an imperative program to work with, but most meaningful programs also include a notion of pure expressions. These expressions contain a combination of one or more values, constants, variables and operators that our library interprets and computes to produce a new value. This process is often referred to as evaluation. As an example, consider a function for squaring a value:

\begin{code}
squareS :: SExp Int32 -> SExp Int32
squareS a = a * a
\end{code}

\noindent Applying the squaring function to a value of $5$ is equaivalent to the mathematical expression $5*5$ which evaluates to $25$.

Expressions are however a deeply embedded type, and evaluation is not the only interpretation that they support. For instance, looking at the type of \codei{square} we see that its a function from a software expression over 32-bit integers to another expressions of the same type. Software expressions like this, when used in software programs, can also be compiled to C. Like the software programs from section~\ref{codesign} have a corresponding type for hardware programs, \codei{SExp} also has a corresponding hardware type for expressions called \codei{HExp}. The squaring function can be implemented using hardware expressions by simply changing its type:

\begin{stub}
squareH :: HExp Int32 -> HExp Int32
squareH a = a * a
\end{stub}

Looking at the implementation for our hardware and software squaring functions we see that they're identical; both software and hardware support a multiplication operator for their expressions. There is no software or hardware specific characteristics in the squaring function, its expression type only need to support multiplication which, in Haskell, comes from its \codei{Num} class of numerical operations:

\begin{code}
square :: (Num (exp a), Type' exp a) => exp a -> exp a
square a = a * a
\end{code}

\noindent Note that the type constraint is slightly different from previous functions, as it makes us of \codei{Type'} instead of \codei{Type}. The former type constraint is designed for functions are parameterized on their expression type, while the latter is used with monads.

In simple expressions, the resulting value is usually one of various primitive types supported by the co-design library, such as signed and unsigned numerical, floating point, and logical; in more elaborate expressions, expressions can have a composite type of smaller, andIn simple settings, the resulting value is usually one of various primitive types, such as numerical, string, and logical; in more elaborate expressions, it can be a complex data type. For instance, it is possible to make an expression over a pair of values, and we can define functions over these pairs as well:

\begin{code}
dist :: (SExp Float, SExp Float) -> (SExp Float, SExp Float) -> SExp Float
dist (x1, y1) (x2, y2) = sqrt (dx**2 + dy**2)
  where
    dx = x1 - x2
    dy = y1 - y2
\end{code}

\noindent \codei{dist} computes the distance between two points in a plane, where points are represented as a pair of coordinates. In order to compile \codei{dist}, we need to get its input from somewhere and do something with its output. A convenient way to do this in software is to use its \codei{fget} function to read standard input, \codei{stdin}, and \codei{printf} to writes the output out into standard output.

\begin{code}
distProg :: Software ()
distProg = do
  a <- fget stdin
  b <- fget stdin
  printf "dist: %d\n" (dist (a, b) (b, a))
\end{code}

Compiling \codei{distProg} produces the following code:

\begin{code}
\end{code}

\noindent Note that there are no pairs in the generated C code, as the different coordinates simply become four separate references to the two input variables.

\subsection{Transmission}

The ability to write software, hardware and generic programs that our co-design library means users can easily experiment with different language implementations of their functions. Most interesting heterogeneous programs does however include a mixture of software and hardware fragments, where the different parts communicate with each other. In the kind of FPGAs with embedded systems that we consider, communication is typically done over an AXI4 interconnect.

Full AXI4 offers a range of interconnects that include variable data and address bus widths, high bandwidth burst and cached transfers, and various other transaction features. The full specification of AXI4 provides great flexibility to users, and provides advanced burst and streaming transactions. For simpler programs, like the examples we have seen so far, there is no need for such advanced features. A lighter interconnect is therefore also specified by AXI4, called AXI4-lite. This subset of the full specification forgoes the more advanced features for a simpler communication model of writing and reading single pieces of data, one at a time.

Five channels make up the whole AXI4-lite specification: the read and write address channels, the read and write data channels, and the write acknowledge channel. We introduce these channels from the processor's perspective. These channels are represented in VHDL as signals, driven by processes that implement the associated handshaking and logic for reading and writing. Signals behave very much like the references found in C, and processes is a kind of function runs automatically once any of its input changes. Signals and process are both supported by our co-design library, and the whole AXI4-lite interface can in fact be implemented within the library. A function that takes a hardware program and hooks it up to an AXI4-lite interconnect is provided by the library:

\begin{code}
axi_light :: HComp a -> HSig (
     Signal Bit       -- ^ Global clock signal.
  -> Signal Bit       -- ^ Global reset signal.
  -> Signal (Bits 32) -- ^ Write address.
  -> Signal (Bits 3)  -- ^ Write channel protection type.
  -> Signal Bit       -- ^ Write address valid.
  -> Signal Bit       -- ^ Write address ready.
  -> Signal (Bits 32) -- ^ Write data.
  -> Signal (Bits 4)  -- ^ Write strobes.
  -> Signal Bit       -- ^ Write valid.
  -> Signal Bit       -- ^ Write ready.
  -> Signal (Bits 2)  -- ^ Write response.
  -> Signal Bit       -- ^ Write response valid.
  -> Signal Bit       -- ^ Response ready.
  -> Signal (Bits 32) -- ^ Read address.
  -> Signal (Bits 3)  -- ^ Protection type.
  -> Signal Bit       -- ^ Read address valid.
  -> Signal Bit       -- ^ Read address ready.
  -> Signal (Bits 32) -- ^ Read data.
  -> Signal (Bits 2)  -- ^ Read response.
  -> Signal Bit       -- ^ Read valid.
  -> Signal Bit       -- ^ Read ready.    
  -> ())
\end{code}

There's a few new types in \codei{axi_light}. The first two types, \codei{HComp} and \codei{HSig}, are both more or less pure hardware programs, but have a type that we can inspect from within Haskell (normally, one cannot inspect the type of functions). The resulting hardware signature can then be compiled to VHDL and loaded into a synthesis tool like Vivado, which will turn it into a physical component which can be put onto the FPGA.



\begin{code}
mmap :: String -> HSig a -> Software (SComp (Soften a))
\end{code}

Text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .x~~"*Weu.
%  d8Nu.  9888c
%  88888  98888
%  "***"  9888%
%       ..@8*"
%    ````"8Weu
%   ..    ?8888L
% :@88N   '8888N
% *8888~  '8888F
% '*8"`   9888%
%   `~===*%"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Concluding Remarks}
\label{ch:conc}

Text.

