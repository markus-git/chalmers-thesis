%!TEX root = ../main.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%       oe    
%     .@88    
% ==*88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
%    88888    
% '**%%%%%%** 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

Over the last few years, the amount of traffic going back and forth between devices in the global communications infrastructure has been increasing at a rapid pace. As the steady growth of mobile users and internet of things devices continues~\cite{ericsson2016}, the amount of mobile traffic is projected to become even greater. In fact, global internet traffic is estimated to grow at an average rate of twenty two percent annually, reaching approximately two hundred and fifty million terabytes per month by the end of 2021~\cite{cisco2016}. For the communication infrastructure, the consequence of such a rapid growth rate has been a sharp increase in the demand for computational power on systems that already run under tight latency constraints and with limited memory~\cite{persson2014}, which means computations have to be efficient.

As important as it is to increase the computational power of embedded systems used in communication, limiting their power consumption presents an equally important issue in their architectural design~\cite{mudge2001}. The trend of trading power for performance cannot continue indefinitely: typical house hold processors of today have a power density of 70W and upwards, which is seven times that of a typical hot plate. Containing the growth in power requires architectural improvements, with specialized computing for specialized tasks. Heterogeneous computing represents an interesting development towards the goal of energy efficient computing, and refers to systems that use more than one kind of processing units. These heterogeneous systems gain their performance and energy efficiency not just by combining several processors, but rather by incorporating different kinds of co-processors that provide specialized processing capabilities to handle a particular task.

Heterogeneous computing present new challenges in software design that are not found in the development for typical homogeneous systems~\cite{kunzman2011}. The multiple processing units present in a heterogeneous system raises all of the issues associated with homogeneous parallel systems, while the heterogeneity in the system gives rise to new issues due to  dissimilarity in system development and capability. The efficiency and computational power of heterogeneous systems thus comes at a cost of increased programming burden in terms of code complexity and portability, as hardware specific code is interleaved with application code to handle any communication between co-processors. Furthermore, the structure of application code typically vary between co-processors and any code written for one therefore requires modification when given a new target. In fact, despite all the advantages heterogeneous systems offer, their use so far has been mostly restricted to hardware programmers.

A substantial amount of research has gone into addressing the challenges of programming for embedded heterogeneous systems, opening them up for programmers without a background in hardware or embedded system design. However, hardware description languages, such as VHDL and Verilog, are still the most commonly used tools, together with C for specific co-processors. These hardware description languages have revolutionized hardware design but suffer from a lack of expressiveness and standardization -- there is a mismatch between description and synthesized hardware. Designers have therefore looked for alternative solutions, where one of the more well-known approaches is synthesis of high-level languages like C~\cite{graphics2008, ghenassia2005}. Compiling high-level languages to a hardware description has great appeal, but finding a translation between the two has however proven to be difficult; sequential programs are often a bad fit for the parallelism inherent to most hardware architectures.

Another group of languages whom I believe have shown success in describing hardware designs are functional languages~\cite{sheeran2005}. Higher-order functional languages in particular, where hardware descriptions are first-class objects, offer a particularly useful abstraction mechanism~\cite{baaij2010, bjesse1998, gill2010}. Another beneficial attribute of these languages is their purity, which enables reasoning about function (de-)composition. \todo{More?}

% Functional languages are however rarely considered for embedded system development, as its difficult to give performance guarantees and resource bounds for its programs.

This thesis presents the first steps towards a functional programming language in which the entire design process of heterogeneous systems can be expressed. I do however intend to go further than solely describing hardware; some components are better described using sequential algorithms directly, rather than having one generated from a hardware description. I also intend to support the necessary design exploration to decide where the boundary between hardware and software should be drawn. \todo{More?}

This thesis consists of two parts. Part I is a general introduction to the field and puts the appended papers into context. Part II contains the appended papers.

% Limit to FPGAs.

\section{Background}

High demands for efficiency under resource constraints have greatly influenced the development of embedded systems used in communications infrastructure, both in terms of programming practice and architecture. Today, we see embedded systems consisting of everything between general purpose processors (GPPs) and application specific integrated circuits (ASICs). GPPs are highly programmable systems but often inefficient in terms of power consumption and performance. In contrast, ASICs implement a fixed function and can therefore provide good power and performance characteristics, but any changes to its functionality requires a new circuit to be designed. 

Processors and ASICs represent two extremes of available architectures, but fortunately there exists several other architectures in between. Field programmable gate arrays (FPGAs) are one such kind, and provides the best of both worlds: they are close to hardware and can be reprogrammed~\cite{bacon2013}. FPGAs typically consists of a large array of configurable logic blocks, connected by programmable interconnects. However, a modern FPGA also contain various discrete components and co-processors, which together with their good performance per Watt ratio, have seen them increasingly used in high-performance, computationally intensive systems~\cite{mcmillan2014}.

\subsection{Difficulties of heterogeneity}
\label{issues}

While modern FPGAs show great promise as a prototypical system for heterogeneous computing, their adoption has been slowed by the fact that they're difficult to program. The logic blocks are usually programmed in a hardware description language, while the embedded processors and accelerators are typically programmed in a low level dialect of C or assembler.  This choice of low level languages is primarily driven by the desire to access the full potential of a processor or its memory system. However, low level languages also forces its developers to focus on low level implementation details rather than the high level specification of the algorithm they are implementing.

Furthermore, in the design of a heterogenous systems there's also concerns related to any dissimilarities between the embedded components that needs to be address. Components may support different instructions, leading to incompatibilities between the code they can execute even if they're both programmed in the same language. Even the implementation of language features, such as functions, may differ between components. In addition to instructions, components may interpret memory in different ways and have access to various cache structures. These differences imply that a heterogeneous system may have components that are similar in architecture, but have low-level differences that lead to variations in performance and power consumption.

One of the benefits of low level code is that it gives programmers fine control over the aforementioned system capabilities and differences thereof. However, this control comes at a cost, as the programmers must exercise this right during the entire design process. So the problem of implementing an algorithm has become a problem of implementing an algorithm for a specific component. In heterogeneous system design, the developer must specify both hardware and software parts, and how they communicate; ideally she would like to experiment with various choices of what to put in hardware and what in software. Low-level languages provide little support for such design exploration, and rewriting code intended for component to another is a major undertaking.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .--~*teu.
%  dF     988Nx
% d888b   `8888>
% ?8888>  98888F
%  "**"  x88888~
%       d8888*`
%     z8**"`   :
%   :?.....  ..F
%  <""888888888~
%  8:  "888888*
%  ""    "**"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Co-Design Language}

Many of the issues in section~\ref{issues} that one faces in heterogenous computing with low level languages like C and HDLs are related to a lack of abstractions. Some of these languages modularity problems come as a direct result of the fine grained control they provide, as it will inevitably tie programs to the architecture of its system. Some issues come as a indirect consequence of the lack of abstractions. For instance, we would like to treat functionality like parallelism as a seperate design decision. In imperative languages like C, this is however quite difficult to do. HDLs contrast this by assuming programs to be parallel by default.

Issues with differences in language design between imperative languages and HDLs can also be ascribed to a lack of abstractions. Ideally, issues of functionality, architecture or parallelism would be treated separately. As being able to do so would allow us to create reusable libraries that provide solutions to these issues, and then combine them in a modular fashion to solve problems.

\subsection{Functional Programming}

Functional programming, as the name implies, is based around the application of functions to its arguments. In this programming style, programs are written as functions that accept input and delivers its result. These functions are themselves defined in terms of smaller functions, which in turn are defined using smaller functions still and, in the end, a function consists of language primitives such as built-in functions, variables or constants. An important distinction between functions in a functional programming language and, say an imperative language like C, is that functions always return the same value when given same arguments. More generally, we say that functional programs have no side effects and their definitions and evaluation are thus separated; functions can safely be evaluated in parallel as long at their data dependencies are satisfied.

A function that accepts other functions as arguments is often referred to as a higher-order function, or a combinator, and provide a useful piece of glue code that lets programmers build complex functions from smaller ones. In Haskell, a functional programming language, a number of such higher-order functions that implement common operations are provided by its standard libraries. One such function is \codei{map} and it can be defined as follows:

\begin{code}
map :: (a -> b) -> [a] -> [b]
map f []     = []
map f (x:xs) = f x : map f xs
\end{code}

The first line specifies the type of \codei{map}, because in Haskell, every function is assigned a static type in an effort to attain safer programs. If you by mistake write a program that tries to multiply some integer by a boolean type, it won't even compile and instead warns you about the type error. As for the types themselves, they are a kind of label that every expression has and states what category of operations that the expression belongs in.

A function's type comes after the \codei{::} sign, and in the case of \codei{map}, tells us that its first argument is a function \codei{f :: a -> b} which, given an argument of type \codei{a}, produces a result of type \codei{b}. In addition to the function, \codei{map} also takes a list \codei{xs :: [a]} of element with type \codei{a}, and returns another list of elements with type \codei{b}. As functions cannot have any side effects, we can already make a guess at what this function does and claim that it applies \codei{f} to every element of \codei{xs}.

The second and third line of \codei{map} validates our earlier guess and lists the full definition of the function. Firstly, it says that given an empty list, shown as \codei{[]}, the result is another empty list as there's simply nothing to apply \codei{f} to. Secondly, in the case where \codei{map} is given a non-empty list \codei{x:xs} where \codei{x} is the list head and \codei{xs} its tail, it applies \codei{f} to \codei{x} and concatenates its result with a recursive call to itself on \codei{xs}.

The other piece of glue code that functional programming languages provides is often referred to as function composition, and enables programs to be glued together. Say that \codei{f} and \codei{g} are two programs, then \codei{g} composed with \codei{f} is written \codei{(g . f)} and is a program that, when applied to its input \cdoei{x}, computes \codei{g (f x)}. In Haskell, we can define function composition as:

\begin{code}
(.) :: (b -> c) -> (a -> b) -> a -> c
(.) g f x = g (f x)
\end{code}

\noindent where parenthesis around the dot implies that function composition is an infix function. 

\subsection{Domain Specific Languages}

A domain specific language (DSL) is a special-purpose language, tailored to a certain problem and captures the concepts and operations in its domain. For instance, a hardware designer might write in VHDL, while a web-designer might write an interactive web-page using JavaScript. Both use a language that is customized to their particular task at hand, and both build programs in a form that is familiar to programmers; VHDL and JavaScript are both examples of a DSL.

DSLs comes in two fundamentally different forms: external and internal. An external DSL is a first-class language, with its own compiler or interpreter, and often comes with its own ecosystem. The previous VHDL and JavaScript examples both fall into this domain of DSLs. Internal DSLs are embedded in a host language, and are often referred to as embedded domain specific languages (EDSLs). Embedded languages can have the look and feel of a stand-alone language, but reuse many parts of the host-language's ecosystem and semantics to lower the cost to develop and maintain them.

Haskell, with its static type system, flexible overloading and lazy semantics, has seen itself as host for a range of EDSLs. For instance, popular libraries for parsing, pretty printing, hardware design and testing have all been embedded in Haskell. These embedded languages come in two different flavors, where the most common one is called shallow embedding and is represented by functions that implements the semantics of the embedded language. The advantage of this embedding is that calculating the value of a computation is very fast, as we just evaluate it to get its value.

Shallowly embedded languages do however perform quite poorly as a technique if we wish to compile the language since its functions return values. To compile an embedded language it is better to use an intermediate representation for its abstract syntax tree. This technique is known as a deep embedding, and lets us inspect a whole computation rather than its result. While the representation of embedded language's syntax tree can become quite unwieldy as the supported syntax grows, one is necessary if we would like to transform the representation, or if we have another back-end than compilation.

\chapter{Co-Design}

\lipsum[5]

\section{Section about Expression}

\lipsum[1]

\begin{code}
square :: SExp Int32 -> SExp Int32
square a = a * a
\end{code}

\lipsum[1]

\begin{stub}
square :: HExp Int32 -> HExp Int32
square a = a * a
\end{stub}

\lipsum[1]

\begin{stub}
square :: (Multiplicative exp, Type' exp a, Num a) => exp a -> exp a
square a = a * a
\end{stub}

\lipsum[1]

\begin{code}
type Point a = (a, a)

pair :: (Expr exp, Type' exp a, Num a) => Point (exp a) -> Point (exp a) -> exp a
pair (a, b) (u, v) = (a + b) * (u + v)
\end{code}

\lipsum[1]

\begin{code}
dotProd :: (Expr exp, Type' exp a, Num a) => Pull exp a -> Pull exp a -> exp a
dotProd xs ys = forLoop n 0 $ \i s -> s + xs!i * ys!i
  where
    n = min (length xs) (length ys)
\end{code}

\lipsum[1]

\begin{stub}
forLoop :: Syntax exp st => exp Length -> st -> (exp Index -> st -> st) -> st
\end{stub}

\lipsum[1]

\begin{code}
zipWith :: Expr exp => (a -> b -> c) -> Pull exp a -> Pull exp b -> Pull exp c
zipWith f xs ys = fmap (uncurry f) (zip xs ys)

sum :: (Expr exp, Type' exp a, Num a) => Pull exp a -> exp a
sum = fold (+) 0
\end{code}

\lipsum[1]

\begin{code}
scProd :: (Expr exp, Type' exp a, Num a) => Pull exp a -> Pull exp a -> exp a
scProd a b = sum (zipWith (*) a b)
\end{code}

\section{Section about Programs}

\lipsum[1]

\begin{code}
hello :: Software ()
hello = printf "Hello world!\n"
\end{code}

\lipsum[2]

\begin{code}
hello :: Software ()
hello = printf "Hello world!\n"
\end{code}

\lipsum[3]

\begin{code}
reverse :: SArr Int32 -> Software ()
reverse arr =
  do for 0 (len `div` 2) $ \ix ->
       do aix <- getArr arr ix
          ajx <- getArr arr (len - ix)
          setArr arr ix         ajx
          setArr arr (len - ix) aix
  where
    len = length arr
\end{code}

\lipsum[4]

\begin{stub}
reverse :: HArr Int32 -> Hardware ()
\end{stub}

\lipsum[5]

\begin{stub}
reverse :: (Arrays m, Expr (Exp m), Type' (Exp m) Int32) =>
  Arr m (Exp m Int32) -> m ()
\end{stub}

\lipsum[6]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .x~~"*Weu.
%  d8Nu.  9888c
%  88888  98888
%  "***"  9888%
%       ..@8*"
%    ````"8Weu
%   ..    ?8888L
% :@88N   '8888N
% *8888~  '8888F
% '*8"`   9888%
%   `~===*%"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Concluding Remarks}
\label{ch:conc}

\lipsum
