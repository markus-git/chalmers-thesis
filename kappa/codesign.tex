%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   .x~~"*Weu.
%  d8Nu.  9888c
%  88888  98888
%  "***"  9888%
%       ..@8*"
%    ````"8Weu
%   ..    ?8888L
% :@88N   '8888N
% *8888~  '8888F
% '*8"`   9888%
%   `~===*%"`
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[../paper.tex]{subfiles}
\begin{document}

\chapter{Co-Design}
\label{codesign}

In section~\ref{embedded} we introduced a simplified version of our co-design, vector and signal languages to illustrate embedded programming in Haskell. We then went on to implement two versions of a FIR filter, one with vectors and one with signals, and showed their compiled C code. For the kind of heterogeneous computing that our co-design library aims to describe it is however not enough to only generate C code; heterogeneous system typically see hardware code interleaved with software code, even components described by the same language can differ by what intrinsic operations they support.

% C is perhaps the most commonly used language for writing software in embedded systems and is as such used by the co-design library as well. For hardware descriptions we use the VHDL language, purely based on the fact it is the hardware description language we are most comfortable with.

Starting with a single Haskell program, our co-design library is designed with three main tasks in mind: generate C code for the software parts, VHDL for the hardware parts, and to generate a combination of software and hardware for the transmission of data between components. Furthermore, the software and hardware programs are both extensible in the sense that they support the addition of new operations to account for differences between components.

\section{Hardware Software Programs}
\label{program}

While the C and VHDL languages different in that one describes software code and the other hardware designs, both languages exhibit an imperative style of programming. As a consequence, our co-design language is built on a deep embedding of monads, as a representation of imperative programs. Monads can be thought of as composable descriptions of computations, that is, they provide a means to connect smaller programs into a single, larger program.

The general idea behind a monadic embedding is that one can view an imperative program as a sequence of instructions to be executed on some machine---which looks similar to programs written in a stateful monad using Haskell's do-syntax. In fact, a stateful program composed with monadic operations can be directly translated into statements in an imperative language.

As an example of the similarities between monads and imperative programs, consider a software program for reversing an array:

\begin{code}
reverseS :: SArr Int32 -> Software ()
reverseS arr =
  for 0 (len `div` 2) $ \ix -> do
    aix <- getArr arr ix
    ajx <- getArr arr (len - ix - 1)
    setArr arr ix ajx
    setArr arr (len - ix - 1) aix
  where
    len = length arr
\end{code}

\noindent Its type signature tells us that it takes an array over 32-bit integers as input and produces a software program---notice that the array type \codei{SArr} is prefixed with an \codei{S} in order to show that it is intended to be used with software statements. The return type is empty as the array is reversed in place.

% The implementation of \codei{reverseS} certainly has the look and feel of a imperative program, sans a few syntactical differences. As a result, the translation of \codei{reverseS} to C code is straightforward: we simply translate each statement to their corresponding statement in C. We can even translate each statement individually since monads take care of the program structure.

% \codei{reverseS} makes use of two array operations for reading and writing to an array, \codei{getArr} and \codei{setArr}, which behave as you would expect them to: \codei{getArr} takes an array and an index, and returns the array's value at that index; \codei{setArr} takes an array, an index and a value, and stores the value in the array.

% For each of these indices, \codei{ix}, it reads two values from the array at the leftmost and rightmost places, offset by the index. Having read and stored these two values into the \codei{aix} and \codei{ajx} variables, they are then put back into the array at the opposite place. This process of flipping pairs of values will eventually result in a reversed array when the two indexed values meet in the middle. 

While the type of \codei{reverseS} is that of a software function, there's nothing software specific about it. For-loops and arrays are both part of most imperative languages, and we could just as well have implemented the same reverse function in hardware. In fact, we can define the hardware version by simply changing the previous function's type signature while keeping its body intact:

\begin{code}
reverseH :: HArr Int32 -> Hardware ()
reverseH arr =
  for 0 (len `div` 2) $ \ix -> do
    aix <- getArr arr ix
    ajx <- getArr arr (len - ix - 1)
    setArr arr ix ajx
    setArr arr (len - ix - 1) aix
  where
    len = length arr
\end{code}

The fact that we are able to define both the \codei{reverseS} and \codei{reverseH} using the same function body hints that having them marked as software and hardware programs was unnecessarily restrictive. After all, the function only requires its language to support for-loops, arrays, and a few numerical operations.

Functions which are not limited to either hardware or software are supported by the co-design language through its hierarchy of type classes. That is, as we used Haskell's \codei{Num} class in section~\ref{embedded} to accept any numerical value in our dot-product, we can also use type classes to accept any language which support the classes' functions. As for the reverse function, which makes use of a for-loop and array operations, we can give it the following type signature:

\begin{code}
reverse :: (Monad m, Arrays m, Control m, TypeM m Int32)
        => Arr m Int32 -> m ()
\end{code}

\noindent In order to make the function language agnostic, the previous array types of \codei{SArr} and \codei{HArr} have been substituted for the generic array type \codei{Arr}, which is parameterized on the monad \codei{m} rather than associated with a specific language. The general idea behind \codei{Arr} is that it will turn into \codei{SArr} or \codei{HArr} when instantiated with their respective monads.

Three new constraints have been introduced, namely the \codei{Arrays}, \codei{Control}, and \codei{TypeM} type classes. Where the first two ensure \codei{m} supports arrays and for-loops, whereas the third one states that a 32-bit integer is a valid type in \codei{m}. The two type classes for \codei{Arrays} and \codei{Control} are defined as follows:

\begin{code}
class Monad m => Arrays m where
  type Arr m
  newArr :: TypeM m a => Exp m Length -> m (Arr m a)
  getArr :: TypeM m a => Arr m a -> Exp m Index -> m a
  setArr :: TypeM m a => Arr m a -> Exp m Index -> a -> m ()

class Monad m => Control m where
  for :: (TypeM m a, Integral a) => Exp m a -> Exp m a -> (Exp m a -> m ())
      -> m ()
\end{code}

\noindent Where each class lists their associated functions and in the case of arrays, the type to use with them. \codei{Exp} represents the expression type associated with \codei{m}. As functions like these are quite common in imperative programs, we provide a short-hand for type classes with common computational functions called \codei{MonadComp}:

\begin{code}
type MonadComp m = (Monad m, References m, Arrays m, Control m)
\end{code}

% The \codei{TypeM} and \codei{Exp} types introduced here are quite similar to the earlier \codei{Type} and \codei{Exp} types from section~\ref{haskell}. In fact, the latter two types are defined in terms of the general ones but applied to the softare monad, as we used them with examples that were compiled to C code.

While the software and hardware monads both support the above array and control operations, the co-design library also provides classes of operations that are only supported by software but not hardware, and vice versa---like processes in hardware or IO in software:

\begin{code}
class HardwareMonad m => Process m where
  process :: [Name] -> m () -> m ()
\end{code}

\noindent Monads still form the base of the class hierarchy, but functions intended for either software or hardware branches also require that \codei{m} is an extension of their respective monads.

At this point we should note that types introduces in this section are slightly different from those found in section~\ref{embedded}. For instance, the array type now has an extra parameter \codei{m}. These differences are the result of our wish to simplify the function types in the earlier section and, in the case of arrays, can be unified by using either \codei{SArr} or \codei{HArr}.

With the new types and classes, we revisit the dot product from section~\ref{embedded} and reimplement it as a generic function:

\begin{code}
dotSeq :: (MonadComp m, TypeM m a, Num a) =>
  Arr m a -> Arr m a -> Program (Exp m a)
dotSeq x y = do
  sum <- initRef 0
  for 0 (min (length x) (length y) $ \ix -> do
    a <- getArr x ix
    b <- getArr y ix
    modifyRef sum $ \s -> s + a * b
  getRef sum
\end{code}

\noindent Aside from its type signature, which now includes the constraints \codei{MonadComp} and \codei{TypeM}, the function is the same as its previous incarnation.

As an example of a larger function, we also implement the full FIR filter:

\begin{code}
firSeq :: (MonadComp m, TypeM m a, Num a) =>
  Arr m a -> Arr m a -> m (Arr m a)
firSeq bs xs = do
  taps <- newArr (length bs)
  ys   <- newArr (length xs)
  for 0 (length bs) $ \ix -> do
    setArr taps ix 0
  for 0 (length xs) $ \ix -> do
    for 1 (length bs) $ \jx -> do
      tmp <- getArr taps (jx - 1)
      setArr taps jx tmp
    x <- getArr xs ix
    setArr taps 0 x
    o <- dotSeq bs taps
    setArr ys ix o
  return ys
\end{code}

\noindent The filter itself is fairly straightforward: inputs are shifted onto an array holding the filter's taps and with each shift one output is calculated by the dot product of the current coefficients and taps. The taps themselves are stored in an array, which is initialized with zeroes.

While the above filter is suitable for a software implementation, it is perhaps not ideal as a hardware design: signal processing in C is often done with arrays over chunks of the input, while a hardware design makes use of signals and processes to drive a continuous filter. In the case of our FIR filter, the change to hardware is however quite small: we simply swap the input and output arrays for signals and change the outer for-loop into a process. Rewriting programs at this scale is a kind of optimization wich we think developers are comfortable with. 

% and as long as layout experimentation, 

% Connecting the FIR filter to a simple main program that supplies an example arrays for its coefficients and input lets us instantiate the code in, for example, the software monad and compile it to C:

% \noindent Note that, in order to make the filter implementation a bit shorter, we given the output signal as an argument to the filter.

% Making the change from generic arrays to signals means that we also have to update the function's body in order to reflect its new types. For signals, the difference is that values are now processed one by one instead of chunks, that is, rather that iterating over an input array we will have to process an input signal and react to its changes in value with a process:

% \begin{code}
% firH N b xs ys = do
%   taps <- initArr (replicate N 0)
%   ys   <- initSignal 0
%   process (xs .: []) $ do
%     for 1 (lit N) $ \jx -> do
%       tmp <- getArr taps (jx - 1)
%       setArr taps jx tmp
%     x <- getSignal xs
%     setArr taps 0 x
%     setSignal ys (dot b taps)
%   return ys
% \end{code}

% \noindent Note that, the arrays used for taps and coefficients are held by signals.

% Connecting the hardware filter to a main program body that supplies it with its inputs and compiling it produces the following VHDL design:

\section{Instructions}
\label{instr}

The co-design language is inspired by the work of~\cite{BjornBenny} and by the Operational Monad~\cite{Operational} and is as such based on a monadic representation of imperative programs. Like our inspiration, these programs capture a monadic computation as an algebraic data type. However, unlike our inspiration, we also take into account the fact that languages support different instructions, types and expressions. The program type is therefore parameterized on its instruction type and a type-level list of types associated with the language:

\begin{code}
data Program instr fs a
\end{code}

\noindent As programs are a deep embedding of monads they instantiate Haskell's monad class:

\begin{code}
instance Monad m => Functor     (Program instr fs)
instance Monad m => Applicative (Program instr fs)
instance Monad m => Monad       (Program instr fs)
\end{code}

The general idea behind our program type is that it allows for instructions to be separated from their sequencing, since an instruction's effect will only depend on its interaction with other instructions. That is, Haskell's monadic syntax ensures that any instructions we use in our programs are sequenced correctly. As a consequence of this separation, the task of implementing a language based on \codei{ProgramT} is the same as writing an interpreter for the language's instructions. In particular, we can define a generic interpreter for programs that maps them to their intended meaning:

\begin{code}
interpret :: (Interp i m fs, HFunctor i, Monad m) => Program i fs a -> m a
\end{code}

\codei{interpret} lifts a monadic interpretation of instructions, which may be of varying types, to a monadic interpretation of the whole program. By using different types for the monad \codei{m}, one can implement different ``back ends'' for programs. For example, interpretation in Haskell's \codei{IO} monad gives a way to \emph{run programs}, while interpretation in a code generation monad can be used to make a \emph{compiler}. The interpretation of an instruction set \codei{i} to the monad \codei{m} is given by \codei{Interp}:

\begin{code}
class Interp instr m fs where
  interp :: instr '(m,fs) a -> m a
\end{code}

Note that \codei{interpret} also require that its instructions are higher-order functors, parameterized on the program monad that they are part of---the instructions here are actually given a list of parameters, but the interpreter is only interested in its first member. There is however nothing stopping us from having instructions with more parameters. The \codei{HFunctor} class of higher-order functors is defined as follows:

\begin{code}
class HFunctor h where
  hfmap :: (forall b . f b -> g b) -> h '(f, fs) a -> h '(g, fs) a
\end{code}

Having instructions be parameterized on the program monad makes it possible to define instruction sets compositionally using, for instance, a technique like Data Types \`{a} La Carte~\cite{DTC}. In the original Operational Monad this could be only done for simple instructions, but with our new program type it can be done even for \emph{control instructions}---instructions that take programs as arguments. Extensible instructions sets are particularly useful for the co-design language, as its hardware and software instructions can be reused for new platforms: we only need to add support for its intrinsic functions.

As an example of how an new instruction can be defined, we introduce the \codei{If} construct:

\begin{code}
data ControlCMD fs a where
  If :: exp Bool -> prog () -> prog () -> ControlCMD (P3 prog exp pred) ()
\end{code}

\noindent Note the use of the type parameter \codei{prog} to refer to sub-programs, \codei{exp} refers to pure expressions, and \codei{pred} refers to type predicates. Although \codei{pred} is not used in the definition of \codei{If}, it is used by other instructions and therefore included here to keep the parameter lists consistent. \codei{P3} is a short-hand for a parameter list of three arguments.

Assuming we have already defined a few other instructions like \codei{If}, we can now make program types that combine several instructions with the \codei{:+:} operator from Data Types \`{a} La Carte; for example:

\begin{code}
type MyProgram = Program (If :+: Reference :+: ...) (P2 Exp Pred)
\end{code}

\noindent The recursive \codei{Program} type then sets the type of sub-programs in \codei{If} to \codei{MyProgram}, with \codei{Exp} and \codei{Pred} used as the expression and predicate types.

In order to use the \codei{interpret} function, \codei{If} needs to be an instance of the \codei{Interp} and \codei{HFunctor} classes. Both \codei{Interp} and \codei{HFunctor} do however distribute over \codei{:+:}, which means that it is possible to interpret any instruction set as long as its individual instructions have instances for the respective classes. The \codei{HFunctor} instance is straightforward:

\begin{code}
instance HFunctor ControlCMD where
  hfmap f (If c thn els) = If c (f thn) (f els)
\end{code}

\noindent Interpretation of \codei{If} in, for example, the \codei{IO} monad could look as follows:

\begin{code}
instance Interp ControlCMD IO fs where
  interp (If b tru fls) = if evalExp b then tru else fls
\end{code}

\noindent Where \codei{evalExp} is an evaluator for the expression language.

\section{Expressions}
\label{expr}

A language embedding based on monads gives users a representation of the statements in an imperative program, but most meaningful programs also include a notion of pure expressions. These expressions contain a combination of one or more values, constants, variables and operators that our co-design library interprets and computes to produce a new value. As a small example, consider a function for squaring a value:

\begin{code}
square :: (Num (exp a), Type exp a) => exp a -> exp a
square a = a * a
\end{code}

\noindent Note that the \codei{Type} constraint on \codei{a} is slightly different than \codei{TypeM} from section~\ref{program}, as \codei{Type} accepts expressions rather than monads for its first argument.

% that before as we use \codei{Type} instead of \codei{TypeM}: the former is designed to be used with expressions rather than monads.

Programming with expressions in our co-design language is evidently quite similar to how its done in regular Haskell. For instance, applying the squaring function to a value of $5$ is equivalent to the mathematical expression $5*5$ which evaluates to $25$. However, the expressions used in our co-design language are deeply embedded, and as we saw in section~\ref{domain}, that means evaluation isn't the only interpretation they support. In fact, by substituting the general \codei{exp} type for either the software or hardware expression type, \codei{SExp} and \codei{HExp}, we can also compile the expression.

In simple expressions, like the above squaring function, the resulting value is usually one of various primitive types such as signed and unsigned numerical, floating point, and logical. However, in more elaborate expressions it can be a complex data type. For example, we can define an expression over a pair of values, and we can define functions over these pairs as well:

\begin{code}
dist :: (SExp Float, SExp Float) -> (SExp Float, SExp Float) -> SExp Float
dist (x1, y1) (x2, y2) = sqrt (dx**2 + dy**2)
  where
    dx = x1 - x2
    dy = y1 - y2
\end{code}

\codei{dist} computes the distance between two points in a plane, where points are represented as a pair of coordinates. The pairs are simply syntactic suger, and no pairs will appear in the generated code.

Expressions provide a number of abstractions that we are accustomed to as functional programmers, like let-bindings:

\begin{code}
class Let exp where
  share :: (Type exp a, Type exp b) => exp a -> (exp a -> exp b) -> exp b
\end{code}

\noindent Abstractions like the let-binding are one of the hallmarks of functional programming and let users avoid unnecessary detail and mundane operations. At the same time, abstractions can complicate the compiler and make it harder to generate efficient code. To circumvent this problem the co-design language makes use of two expression types: one with only primitive operations that is easy to compile, and one with features like let-bindings that is easy to program with.

In order to avoid having two separate instances of each interpretation for expressions, which is tedious and error-prone task, we provide an elaboration from the feature rich expressions into program snippets over the primitive expressions:

\begin{code}
elaborateSExp :: SExp a -> Program SIns (P2 CExp SType) (CExp a)
\end{code}

\noindent Where \codei{SIns} and \codei{SType} are the software instruction set and type predicate, and \codei{CExp} are the primitive expressions. The program wrapping is necessary as, for instance, let-bindings are translated into references. Fortunately, programs are regular monads like any other, we can use the \codei{interpret} function to elaborate entire software programs, assuming we use \codei{elaborateSExp} to give an \codei{Interp} instance.

\begin{code}
elaborateSoft :: Software a -> Program SIns (P2 CExp SType) a
elaborateSoft = interpret
\end{code}

An evaluator for software programs can thus be defined by stringing together the elaboration of expressions with an interpreter for programs:

\begin{code}
runSoft :: Software a -> IO a
runSoft = interpert . elaborateSoft
\end{code}

This approach to expressions has several benefits: the translation is typed, which rules out many potential errors, and is easier to write than a complete translation into source code; the low-level expression type is reusable, and can be shared as an elaboration target between multiple high-level expression types.

\section{Components}

The ability to write generic programs that our co-design library means users can easily experiment with different language implementations of their functions. Most interesting heterogeneous programs does however include a mixture of software and hardware fragments, where the different parts communicate with each other. In the kind of FPGAs with embedded systems that we consider, communication is typically done over an AXI4 or AXI4-lite interconnect. 

Full AXI4 offers a range of interconnects that include variable data and address bus widths, high bandwidth burst and cached transfers, and various other transaction features that makes it useful for streaming. A lighter version of the AXI4 interconnect is offered through AXI4-lite, which is a subset of the full specification that forgoes the streaming features for a simpler communication model that writes and reads data one piece at a time. While full AXI4 certainly has its uses, there is no need for such features in the examples we have shown so far. The lighter interconnect offered by AXI4-lite is a better fit for our examples and will be the focus in this section.

% Although burst writing of arrays is desirable they are usually small enough to not impact performance.

Five channels make up the bulk of the AXI4-lite specification: the read and write address channels, the read and write data channels, and the write acknowledge channel. These five channels are represented in VHDL as signals, driven by processes that implement the associated handshaking and logic for reading and writing. Signals behave very much like the references found in C, and processes is a kind of function runs automatically once any of its input changes. Both signals and processes are supported by our co-design library, and the whole AXI4-lite interface is in fact implemented within the co-design library:

% ---while the full AXI4 interface could certainly be implemented as well, we have yet to do so.

\begin{code}
axi4lite ::
  => Signature a
  -> Signature (
          Sig (Bits 32) -- Write address.
       -> Sig (Bits 3)  -- Write channel protection type.
       -> Sig Bit       -- Write address valid.
       -> Sig Bit       -- Write address ready.
       -> Sig (Bits 32) -- Write data.
       -> Sig (Bits 4)  -- Write strobes.
       -> Sig Bit       -- Write valid.
       -> Sig Bit       -- Write ready.
       -> Sig (Bits 2)  -- Write response.
       -> Sig Bit       -- Write response valid.
       -> Sig Bit       -- Response ready.
       -> Sig (Bits 32) -- Read address.
       -> Sig (Bits 3)  -- Protection type.
       -> Sig Bit       -- Read address valid.
       -> Sig Bit       -- Read address ready.
       -> Sig (Bits 32) -- Read data.
       -> Sig (Bits 2)  -- Read response.
       -> Sig Bit       -- Read valid.
       -> Sig Bit       -- Read ready.    
       -> ())
\end{code}

\codei{axi4lite} accepts a hardware component and automatically connects it to an AXI4-lite interconnect, moving values between the component and the read and write channels of the interconnect. The connected component can then be loaded into a synthesis tool like Vivado~\cite{feist2012}, which results in a physical design that can be put onto, for example, the logic blocks of an FPGA.

% A function called \codei{axi_ligth} takes a hardware program and hooks it up to an AXI4-lite interconnect, and the wrapped hardware component can then be compiled to VHDL and loaded into a synthesis tool like Vivado, which will turn it into a physical design that can be put onto the FPGA.

With a component on the hardware we can interface with it using portmaps as usual, but with the physical address of the component in hand we can also memory-mapped I/O to access it from software. Having memory-mapped a component causes them to share address space with the memory of whatever software program they are running in. That is, a memory-mapped component can be reached from software by simply writing to and reading from pointers.

% The components monitor a processors address buss and, whenever an address for component is accessed, they forward the request to the address component.

The co-design library provides a function called \codei{mmap} that preforms the necessary memory-mapping when given a component and its address:

\begin{code}
mmap :: String -> Component a -> Software (Pointer (Soften a))
\end{code}

\noindent Note that the resulting memory pointer has ``soften'' its type, which replaces hardware types like signals with their corresponding type in software. The translation between hardware and software types is described by the type family \codei{Soften}:

\begin{code}
type family Soften a where
  Soften ()           = ()
  Soften (Sig a -> b) = SRef a -> Soften b
  --dotLine
\end{code}

At this point, we have provided a means to load a hardware component onto, for instance, an FPGA's logic blocks and then have it memory-mapped into software. The remaining step is a method to call the component and get its result. In order to call the component, we must first construct a list of argument using these three functions:

\begin{code}
nil   :: Argument ()
(:>)  :: SType a => SRef a -> Argument b -> Argument (SRef a -> b)
(:>>) :: SType a => SArr a -> Argument b -> Argument (SArr a -> b)
\end{code}

\noindent Where \codei{nil} creates an empty argument list, and the two infix functions \codei{(:>)} and \codei{(:>>)} extends an argument list with a reference and an array, respectively. The argument list itself is a typed heterogeneous list that ensures we use the expected number of arguments and that all intermediate types match.

Calling a component once we have our arguments is done by using \codei{call}:

\begin{code}
call :: Pointer a -> Argument a -> Software ()
\end{code}

\noindent Which goes through the argument list and writes each value that's marked as an input, while those marked as outputs store the result read from the component.

As an example of offloading a function to hardware, we will revisit the dot product from section~\ref{embedded} and offload that. Recall that our sequential version of the dot product had the following type signature:

\begin{code}
dotSeq :: (MonadComp m, TypeM m a, Num a) => Arr m a -> Arr m a
  -> Program (Exp m a)
\end{code}

\noindent The above type signature is however not something we can normally inspect from within Haskell, so before the function can be hooked up to an AXI4-lite interconnect and put onto hardware we must first give a signature that can be read by other functions. Seeing as the function accepts two input arrays and produces an expression as output, we can give it the following signature:

% \noindent Normally we cannot inspect the dot products type from within Haskell, so before we can hook up the function to an AXI4-lite interconnect we need to give it a type signature that we can read:

\begin{code}
dotSig :: Signature (HArr Int32 -> HArr Int32 -> Signal Int32 -> ())
dotSig = inputArr 3 $ \a -> inputArr 3 $ \b -> returnC $ dotSeq a b
\end{code}

\noindent Where \codei{inputArr} adds an array of a given length to the signature, and \codei{returnC} rounds of the signature with a signal for the dot product's output.

Signatures like \codei{dotSig} can be inspected by other functions from within Haskell. As a result, they can be connected to other components and hooked up to an interconnect. One such interconnect is the previously mentioned AXI4-lite interconnect, for which the co-design language provides a function called \codei{compileAXI4lite} that automatically connects a signature and outputs the resulting hardware design. Applying the compiler to our \codei{dotSig} we get a design that can be fed into a hardware synthesizers, which in turn produces a physical implementation of the dot-product that we can put onto hardware.

In order to communicate with our offloaded dot-product from software we need its physical address from the synthesis tool, which is ``0x4C300000'' in our case. In addition to getting the component's address, we also need to connect to our component from software with \codei{mmap} and create the two arrays and reference that will hold the dot-products inputs and output. Having done all these steps, we can call the dot-product and read its result from the reference. A program that performs these can be written as:

\begin{code}
program :: Software ()
program = do
  dot <- mmap "0x4C300000" dotSig
  arr <- initArr [1 .. 3]
  brr <- initArr [3 .. 6]
  res <- newRef
  call dot (arr :>> brr :>> res :> nil)
  val <- getRef res
  printf "%d\n" val
\end{code}

\noindent Which will call the component and print its result to standard output.

\section{Vectors}

Sequential programs in the co-design language makes use of its array type to express array and vector computations with mutable updates. These arrays gives a designer full control over their allocation and assignment, but do so through a low-level and imperative interface. As we saw in section~\ref{embedded}, some functions are better expressed in a compositional manner than as a sequential program. To facilitate the design of array and vector functions, we provide the vector language. 

A typical vector computation starts with a ``manifest'' vector, that is, a vector which refers directly to an array in memory. Vector operations are then applied, where these operations are overloaded to accept any ``pully'' vector with another ``pully'' vector as result. Then, once the various vectors have been constructed, they are assembled into a ``pushy'' vector and written to memory, resulting in a new ``manifest'' vector.

The various names for ``manifest'', ``pully'' and ``pushy'' draw inspiration from the Pan language~\cite{} and push arrays~\cite{}, the ideas of which our vectors are based on. Manifest vectors are more often than not immutable arrays provided by the co-design language, as such arrays have an representation in memory. A Push vector on the other hand is not stored in memory, but is rather represented as a function:

\begin{code}
data Pull exp a where
  Pull :: exp Length -> (exp Index -> a) -> Pull exp a
\end{code}

\noindent Where \codei{Length} and \codei{Index} are both aliases for 32-bit integers.

A pull vector consists of a length---the number of elements in the vector---and a function that given an index in the vector returns an element. Furthermore, pull vectors are designed in such a way that all operations fuse together without creating any intermediate structures in memory, a property which is often referred to as vector fusion.

% A pull vector is not monadic, as it is restricted to pure expressions for computing its elements. 

Push vectors go in the opposite direction of pull vectors, and give us control over a vectors evaluation to the producers rather than the consumer. That is, pull vectors have a representation that supports nested writes to memory and fusion of operations. Push vectors are represented as:

% This differences is reflected in the representation of pull vectors as they include monadic effects:

\begin{code}
data Push m a where
  Push :: Exp m Length -> ((Exp m Index -> a -> m ()) -> m ()) -> Push m a
\end{code}

A push vector consists of a length, like pull vectors, but their function describes how elements are evaluated rather then how they are fetched. As such, they are parameterized on the type \codei{m} rather than an expression type; \codei{Exp} is associated type, like \codei{Arr} from section~\ref{codesign}, and refers to the expression type associated with \codei{m}. The general idea is to instantiate \codei{m} as either the co-design's two languages and have the push vector's function write each element to an array.

As an example, we define a function that computes the sum of the square of the numbers from one to $n$:

\begin{code}
squares :: (Num a, Type exp a) => exp a -> exp a
squares n = sum $ map (\x -> x * x) (1 ... n)
\end{code}

\noindent Note that no vector occurs in the function's type, but they are used internally to compute the result: the infix function \codei{(...)} constructs a pully vector with values from one to $n$, to which a mapping is applied that squares each element. The vector is then converted into a push vector and summed up into a single value.

Each vector type has a different set of operations associated with it, and these operations are chosen in such a way that each vector type only supports those operations which can be performed efficiently for that type. In many cases, the vector type is guided by the types of the operations involved, and follows the typical pattern of a manifest vector being turned into a pull vector, which turns into a push vector, which is then written to memory and turns back into a manifest vector. There are however cases where its preferable to ``skip'' parts of the cycle. For instance, the \codei{squares} function starts with a pull vector rather than a manifest vector.

The functions associated with each kind of vector are overloaded in the kind of vectors they accept: an operation for a pull vector will support the use of any ``pully'' vector type. For instance, the \codei{sum} function used in the above \codei{squares} is defined as follows:

\begin{code}
sum :: (Pully exp vec a, Type exp a, Num a) => vec -> a
sum = fold (+) 0
\end{code}

\noindent Where \codei{Pully} summaries the qualities a vector needs to fulfill for it to be considered ``pully''.

As both pull and push vectors are represented by functions, they are't compiled to programs as per say. Rather, those vectors are turned into manifest vectors which have a direct representation in both software and hardware. This, together with the overloading of vector types, enables programs to seamlessly incorporate vectors.

\section{Signal processing}

\label{signals}

While the imperative style of programming of the co-design language is already convenient for software realization, section~\ref{embedded} showed that compositional descriptions can yield a better intuition of what constitutes a program. The vector language was therefore introduced and provides a comfortable syntax for composing sub-components to form larger, array based programs. For functions that involve a notion of time, or are simply better expressed through streaming directly rather than being converted into a function over arrays, we provide the signal language.

% TODO: last sentence above is bad.

The signal language is based on the concept of signals: possibly infinite sequences of values in some pure expression language, given by the type \codei{Sig}. Conceptually, signals can be thought of as infinite lists. Unlike lists however, a signal is not a first-class value and cannot be nested---we cannot construct a signal over other signals.

Programming of signals is done compositionally, and a set of functions is provided to support the composition of new signals from existing ones. That is, a signal program is a collection of mutually recursive signal functions, each built from repeating values or other signals. For instance, some of the supplied combinatorial functions include:

\begin{code}
repeat :: pred a => exp a -> Sig exp pred a

map :: (pred a, pred b) => (exp a -> exp b)
  -> Sig exp pred a -> Sig exp pred b

zipWith :: (pred a, pred b, pred c) => (exp a -> exp b -> exp c)
  -> Sig exp pred a -> Sig exp pred b -> Sig exp pred c
\end{code}

\noindent \codei{repeat} creates a signal by repeating some value; \codei{map} applies a function to each value of a signal; \codei{zipWith} joins two signals element-wise using then given function. The general idea behind is that every ground type, as permitted by the expression language, is lifted to operate element-wise over signals. This can either be done by redefining standard Haskell functions, as we have done with \codei{repeat}, \codei{map} and \codei{zipWith}, or through type classes like \codei{Num}---for classes that cannot be instantiated with signals we provide a new class that mimics the original.

So far the functions we shown are combinatorial. Most interesting examples of signal processing does carry some form of state. For this reason, a sequential operator is also provided by the signal language:

\begin{code}
delay :: pred a => exp a -> Sig exp pred a -> Sig exp pred a
\end{code}

\noindent \codei{delay} prepends a value to a signal, delaying its original output by one time instant. Note that \codei{delay} introduces the notion of a \emph{next time step}, making time enumerable.

While these functions may appear innocent, \codei{repeat}, \codei{map} and \codei{zipWith} can express most combinatorial signal networks, while the combination of \codei{delay} with feedback can describe any kind of sequential signal network. For instance, we can define a parity checker as:

\begin{code}
parity :: Sig exp pred Bool -> Sig exp pred Bool
parity inp = out where
  out = zipWith xor (delay false out) input
\end{code}

As a larger example of feedback, we implement a infinite impulse response (IIR) filter, which comprises the second primary type of digital filters used in digital signal processing applications and, unlike FIR filter in section~\ref{embedded}, contains feedback.

An IIR filter is typically described and implemented in terms of a difference equation:

\begin{equation}
y_{n} = {1 \over a_{0}} \: \cdot \: \left( \sum_{i=0}^{P} b_{i} \cdot x_{n-i} \: - \: \sum_{j=1}^{Q} a_{j} \cdot y_{n-j} \right)
\end{equation}
\vspace{1mm}

\noindent Where $P$ and $Q$ are the feed-forward and feedback filter orders and $a_{j}$ and $b_{i}$ are the filter coefficients. Note that $a_{0}$ is used in the outer division and isn't part of the feedback sum.

Examining the above equation we see that a IIR filter loosely consists of two FIR filters, where the second filter has an extra delay and is recursively defined on its output. As such, we can define the filter as:

\begin{code}
iir :: (Fractional a, Num a, pred a) => exp a -> [exp a] -> [exp a]
  -> Sig exp pred a -> Sig exp pred a
iir a0 as bs x = y
  where
    y = (1 / repeat a0) * (upper x - lower y)
    upper = fir bs
    lower = fir as . delay 0
\end{code}

A circular definition of \codei{y} is possible thanks to the \codei{delay} operator, which ensures a productive network as each output only depends on previous input. In general we have that recursively defined signals introduce feedback, while recursion over other Haskell values like lists can be used to build repeating graphs structures.

This behavior of \codei{delay} implies that we can distinguish values that have and haven't been delayed, which is something that's normally not possible to do in Haskell---as being able to observe the sharing of \codei{y} will, by definition, break any referential transparency. In fact, the internals of \codei{delay} makes use of a restricted form of observable sharing~\cite{claessen1999, gill2009}. This allows us to turn signal functions like the above IIR filter, which describes a network of operations, into a directed graph. Any sharing is then visible as edges in the graph, connecting nodes over its operations.

A graph representation of a signal network enables us to check for cycles, order the nodes, and, finally, compile them into programs. A regular program does however not capture the streaming nature of a signal function, as it represents an action we can run in order to produce a value. So instead of compiling to program directly, the signal language translates a signal into an co-iterative stream~\cite{caspi1998}.

Co-iteration consists of associating to each stream an initial state and a transition function from old state to a pair of a value and a new state. The benefit of this approach is that it allows us to handle infinite data types, like streams, in a strict and efficient way. A slightly modified version of co-iterative streams is provided by the signal language, where state is represented implicitly by the \codei{Program} monad:

\begin{code}
data Stream instr exp pred a where
  Stream :: Program instr (P2 exp pred) (Program instr (P2 exp pred) a)
    -> Stream instr exp pred a
\end{code}

\noindent The outer monad is used to initialize the stream, where the result of the initialization is another program that produces the output values.

The compiler for signal functions is then given as transformation from a function over signals to a function over streams:

\begin{code}
compileF1 :: (Sig exp pred a -> Sig exp pred b)
  -> IO (Str instr exp pred a -> Str instr exp pred b)
\end{code}

\noindent Where the \codei{IO} type is a result of observable-sharing.

\end{document}
